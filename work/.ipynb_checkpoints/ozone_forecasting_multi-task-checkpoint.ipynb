{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es recomendable agregar la extensión jupyter-navbar.  \n",
    "> git clone https://github.com/shoval/jupyter-navbar.git  \n",
    "> cd jupyter-navbar  \n",
    "> python setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notas\n",
    "* Las fechas en el dataset corresponden a cuando se obtuvieron los datos.\n",
    "* Las fechas en los ejemplos corresponden a la predicción Y\n",
    "  \n",
    "  \n",
    "* Invierno: 01 de Mayo al 31 de Agosto.   05-01    -    08-31\n",
    "* Verano: 01 de Noviembre al 31 de Marzo. 11-01    -    03-31\n",
    "\n",
    "* Las Condes:\n",
    "    * V2004 - V2013 (2014)\n",
    "    * mean  76.176741\n",
    "    * std   20.619374\n",
    "    * std/mean 0.2706780800717111\n",
    "    * q0.5  75.557200 -> 76\n",
    "    * q0.75 89.000000 -> 89\n",
    "\n",
    "* Independencia:\n",
    "    * V2010 - V2017 (2018)\n",
    "    * mean  44.780118\n",
    "    * std   15.145352\n",
    "    * std/mean 0.3382159913022114\n",
    "    * q0.5  44.195900 -> 46\n",
    "    * q0.75 54.333300 -> 56\n",
    "\n",
    "* Parque O'Higgins\n",
    "    * V2009 - V2016 (2017)\n",
    "    * mean  51.059853\n",
    "    * std   14.533567\n",
    "    * std/mean 0.2790661935426552\n",
    "    * q0.5  51.000000 -> 51\n",
    "    * q0.75 60.006675 -> 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3382159913022114"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "14.533567/51.059853"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import set_random_seed\n",
    "np.random.seed(123)\n",
    "set_random_seed(2)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.artist import Artist\n",
    "import plotly.plotly as py\n",
    "from plotly.offline import init_notebook_mode, enable_mpl_offline, iplot_mpl, iplot\n",
    "import cufflinks as cf\n",
    "from datetime import datetime\n",
    "\n",
    "from ipywidgets import widgets, interactive, interact\n",
    "from IPython.display import Javascript, display\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, LSTM, Dropout, TimeDistributed\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K\n",
    "\n",
    "from hashlib import md5\n",
    "\n",
    "#import utils\n",
    "#from utils import gpr_invierno, gpr_verano\n",
    "\n",
    "\n",
    "\n",
    "#init_notebook_mode(connected=True)\n",
    "#cf.go_offline(connected=True)\n",
    "#enable_mpl_offline()\n",
    "GRAPH_IS_SET = False\n",
    "\n",
    "ROOT = \"./\"\n",
    "MODELS_FOLDER = \"models\"\n",
    "\n",
    "try:\n",
    "    os.mkdir(MODELS_FOLDER)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones Generales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_station(STATION):\n",
    "    if STATION == \"Las_Condes\":\n",
    "        FILTER_YEARS = [2004, 2013]\n",
    "        DEFAULT_THETA = 89\n",
    "    elif STATION == \"Independencia\" or STATION == \"Independencia2019\":\n",
    "        FILTER_YEARS = [2010, 2017]\n",
    "        DEFAULT_THETA = 54\n",
    "    elif STATION == \"Parque_OHiggins\" or STATION == \"POH_full\":\n",
    "        FILTER_YEARS = [2009, 2016]\n",
    "        DEFAULT_THETA = 60\n",
    "\n",
    "    \n",
    "    return STATION, FILTER_YEARS, DEFAULT_THETA\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_graph():\n",
    "    if \"-f\" not in sys.argv:\n",
    "        return False\n",
    "    global GRAPH_IS_SET\n",
    "    if GRAPH_IS_SET == False:\n",
    "        init_notebook_mode(connected=True)\n",
    "        cf.go_offline(connected=True)\n",
    "        enable_mpl_offline()\n",
    "        GRAPH_IS_SET = True\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dataset(STATION):\n",
    "    if STATION == \"Las_Condes\" or STATION == \"CONDES 4F\":\n",
    "        CSV = \"data/dump-Las_Condes_2018-04-12_230000-verano.csv\"\n",
    "    \n",
    "    elif STATION == \"Independencia\":\n",
    "        CSV = \"data/dump-Independencia_2018-04-12_230000-verano.csv\"\n",
    "    elif STATION == \"Independencia2019\":\n",
    "        CSV = \"data/dump-Independencia_2019-06-14_230000-verano.csv\"\n",
    "    \n",
    "    elif STATION == \"Parque_OHiggins\":\n",
    "        CSV = \"data/dump-Parque_OHiggins_2019-06-12_230000-verano.csv\"\n",
    "    elif STATION == \"POH_full\":\n",
    "        CSV = \"data/dump-Parque_OHiggins_2019-06-14_230000-verano.csv\"\n",
    "    \n",
    "        \n",
    "    DS = pd.read_csv(ROOT+CSV)\n",
    "    #DS = pd.read_csv(ROOT+\"data/dump-Independencia_2018-04-12_230000-verano.csv\")\n",
    "    DS[\"registered_on\"] = pd.to_datetime(DS[\"registered_on\"])\n",
    "    DS.set_index(\"registered_on\",inplace=True)\n",
    "    return DS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HourMax\n",
    "Para obtener la hora a la que ocure el máximo de un atributo en cada día."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HourMax(feature,DF):\n",
    "    maxf = \"hm\"+feature\n",
    "    \n",
    "    fecha = DF.index.hour\n",
    "    fecha.tolist()\n",
    "    DF[maxf] = fecha.tolist()\n",
    "    \n",
    "    g1 = DF.groupby(pd.Grouper(freq=\"D\"))\n",
    "    \n",
    "    hmaxdiaria = []\n",
    "    \n",
    "    for fecha, group in g1:\n",
    "        group = group[[feature,maxf]].dropna()\n",
    "        if not group.empty:\n",
    "            ar = group.values.tolist()\n",
    "            m = max(ar)\n",
    "            hourmax = m[1]\n",
    "            hmaxdiaria.append(hourmax)\n",
    "        else:\n",
    "            hmaxdiaria.append(np.nan)\n",
    "\n",
    "    return np.array(hmaxdiaria)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## precalcular_agregados\n",
    "Precalcula los agregados de los datos que se tienen.  \n",
    "Esto tarda por lo que el resultado es guardado en un archivo.  \n",
    "Se ejecuta la función inmediatamente para asi tener la variable PRECALC disponible como parametro por defecto para cuando se necesite.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precalcular_agregados(STATION,REMAKE=False):\n",
    "    try:\n",
    "        if REMAKE == False:\n",
    "            agdf = pd.read_csv(ROOT+\"precalcs/precalc_agregados_%s.csv\"%STATION)\n",
    "            agdf = agdf[ agdf.columns[1:] ]\n",
    "        else:\n",
    "            pd.read_csv(\"nunca_nadie_me_encontrara.nunca_jamas\")\n",
    "    except:\n",
    "        df = import_dataset(STATION)\n",
    "        agdf = pd.DataFrame()\n",
    "        \n",
    "        agregados = []\n",
    "        \n",
    "        # Calculo de la hora maxima de atributos en un día\n",
    "        for feat in df.columns:\n",
    "            maxf = \"hm\"+feat\n",
    "            print(\"    For: \",feat)\n",
    "            hmaxdiaria = HourMax(feat, df)\n",
    "            print(\"        len(%s), \"%maxf, len(hmaxdiaria))\n",
    "            #agregados.append( (maxf, hmaxdiaria) )\n",
    "            agdf[maxf] = hmaxdiaria\n",
    "        \n",
    "        agdf.to_csv(ROOT+\"precalcs/precalc_agregados_%s.csv\"%STATION)\n",
    "        \n",
    "    \n",
    "    return agdf\n",
    "\n",
    "PRECALC = precalcular_agregados(\"Las_Condes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## precalcular_eventos\n",
    "Precalcula los eventos criticos del ozono.  \n",
    "Un evento crítico es cuando el promedio del ozono en una ventana de 8 horas supera un valor (theta) determinado.\n",
    "Entrega la cantidad de eventos criticos en un día y en otra columna si ocurrió un evento o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def precalcular_eventos(theta,STATION,REMAKE=False):\n",
    "    print(\"Precalculandos Eventos Criticos ...\")\n",
    "    try:\n",
    "        if REMAKE == False:\n",
    "            ecdf = pd.read_csv(ROOT+\"precalcs/precalc_ECat%.2f_%s.csv\"%(theta, STATION), index_col=\"registered_on\", parse_dates=True)\n",
    "            ecdf = ecdf.asfreq(\"D\")\n",
    "        else:\n",
    "            pd.read_csv(\"nunca_nadie_me_encontrara.nunca_jamas\")\n",
    "    except:\n",
    "        df = import_dataset(STATION)\n",
    "        \n",
    "        g1 = df.groupby(pd.Grouper(freq=\"D\"))\n",
    "    \n",
    "        eventos = []\n",
    "        dates = []\n",
    "        for fecha, group in g1:\n",
    "            #print(\"========\")\n",
    "            #print(group[\"O3\"])\n",
    "            group = group[[\"O3\"]].dropna().asfreq(\"h\")\n",
    "            count = 0\n",
    "            #print(\"--\")\n",
    "            \n",
    "            if len(group) >= 8:\n",
    "                ###print(group)\n",
    "                for fecha2 in group.index[:-7]:\n",
    "                    i,f = fecha2, fecha2 + np.timedelta64(7,\"h\")\n",
    "                    gmean = group[i:f].mean()[0]\n",
    "                    if gmean >= theta:\n",
    "                        count += 1\n",
    "                if count > 0:\n",
    "                    ec = 1\n",
    "                else:\n",
    "                    ec = 0\n",
    "                ###print(fecha, count, ec)\n",
    "                eventos.append( [count, ec] )\n",
    "            else:\n",
    "                ###print(fecha, np.nan)\n",
    "                eventos.append( [np.nan, np.nan])\n",
    "            dates.append(fecha)\n",
    "        eventos = np.array(eventos)\n",
    "        ecdf = pd.DataFrame(eventos, columns=[\"countEC\",\"EC\"])\n",
    "        ecdf[\"registered_on\"] = np.array(dates)\n",
    "        ecdf = ecdf.set_index(\"registered_on\")\n",
    "        #print(ecdf[\"EC\"])\n",
    "        \n",
    "        ecdf.to_csv(ROOT+\"precalcs/precalc_ECat%.2f_%s.csv\"%(theta, STATION), index=True)\n",
    "\n",
    "    return ecdf\n",
    "\n",
    "asdf = precalcular_eventos(61,\"Independencia\",REMAKE=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import_merge_and_scale\n",
    "Importa el dataset.  \n",
    "Agrega los datos AGREGADOS. Ej. la hora a la que ocurre el máximo de un atributo.  \n",
    "Y escala los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_merge_and_scale(Config, verbose=True, SCALE = True):\n",
    "    STATION = Config[\"STATION\"]\n",
    "    AGREGADOS = Config[\"AGREGADOS\"]\n",
    "    TARGET = Config[\"TARGET\"]\n",
    "    THETA = Config[\"THETA\"]\n",
    "    PRECALC = Config[\"PRECALC\"]\n",
    "    SHIFT = Config[\"SHIFT\"]\n",
    "    PAST = Config[\"PAST\"]\n",
    "    SCALER = Config[\"SCALER\"]\n",
    "    FILTER_YEARS = Config[\"FILTER_YEARS\"]\n",
    "    IMPUTATION = Config[\"IMPUTATION\"]\n",
    "    if \"GRAPH\" in Config:\n",
    "        GRAPH = Config[\"GRAPH\"]\n",
    "        if GRAPH == True:\n",
    "            for_graph()\n",
    "    #PREDICTED_CE = Config[\"PREDICTED_CE\"]\n",
    "    \n",
    "    \n",
    "    #SCALE = True\n",
    "    \n",
    "    vprint = print if verbose else lambda *a, **k: None\n",
    "    \n",
    "    \n",
    "    print(\"Importing Dataset and Scaling...\")\n",
    "    DS = import_dataset(STATION)\n",
    "    vprint(\"    Dataset imported.\")\n",
    "    \n",
    "    vprint(\"    Adjuntando Agregados Precalculados.\")\n",
    "    agregados = []\n",
    "\n",
    "    if AGREGADOS == [\"ALL\"]:\n",
    "        AGREGADOS = DS.columns\n",
    "    \n",
    "    for feat in AGREGADOS:\n",
    "        maxf = \"hm\"+feat\n",
    "        vprint(\"        Adding \",maxf)\n",
    "        hmaxdiaria = PRECALC[maxf]\n",
    "        agregados.append( (maxf, hmaxdiaria) )\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Agrupar por maximos diarios\n",
    "    gp = DS.groupby(pd.Grouper(freq='D'))\n",
    "    df = gp.aggregate(np.max)\n",
    "    #df = DS#.asfreq(\"H\")\n",
    "    \n",
    "    #TARGET_MASK = np.isnan(df[TARGET]) != True\n",
    "    \n",
    "    if IMPUTATION:\n",
    "        if IMPUTATION != \"mean\":\n",
    "            df = df.fillna(method=IMPUTATION)\n",
    "        else:\n",
    "            df = df.fillna(df.mean())\n",
    "    \n",
    "    for name, data in agregados:\n",
    "        df[name] = data.values\n",
    "    \n",
    "    # Agregando eventos criticos\n",
    "    ecdf = precalcular_eventos(THETA, STATION, REMAKE=False)\n",
    "    df = pd.concat([df, ecdf],axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Agregar si O3 > THETA\n",
    "    df[\"O3btTHETA\"] = (df[\"O3\"].dropna() >= THETA)*1.\n",
    "    \n",
    "    #print(df)\n",
    "    \n",
    "    \n",
    "    #Scaler - normalize\n",
    "    if SCALE:\n",
    "        #min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        #miScaler = preprocessing.StandardScaler()\n",
    "        miScaler = SCALER()\n",
    "        norm_data = pd.DataFrame(miScaler.fit_transform(df), columns=df.columns, index=df.index)\n",
    "        #norm_data[\"EC\"] = ecdf[\"EC\"].values\n",
    "        vprint(\"    Dataset Scaled.\")\n",
    "    else:\n",
    "        #ONLY FOR TEST\n",
    "        norm_data = df\n",
    "        for i in range(0,5):\n",
    "            print(\"\")\n",
    "        print(\"    ===========================================================\")\n",
    "        print(\"    ==================DATASET SIN ESCALAR - OJO================\")\n",
    "        print(\"    ===========================================================\")\n",
    "    \n",
    "    #Scaler para el Y\n",
    "    #Yscaler = preprocessing.MinMaxScaler()\n",
    "    #Yscaler = preprocessing.StandardScaler()\n",
    "    Yscaler = SCALER()\n",
    "    Yscaler.fit(df[TARGET].values.reshape((-1,1)))\n",
    "    \n",
    "    #print(norm_data[\"y\"])\n",
    "    if TARGET in [\"EC\",\"O3btTHETA\"]: #Feature booleana\n",
    "        norm_data[\"y\"] = df[TARGET].values.reshape((-1,1))\n",
    "    else:\n",
    "        norm_data[\"y\"] = Yscaler.transform( df[TARGET].values.reshape((-1,1)) )\n",
    "    #print(df)\n",
    "\n",
    "    #Scaler para datos horarios\n",
    "    #h24scaler = preprocessing.MinMaxScaler()\n",
    "    #h24scaler = preprocessing.StandardScaler()\n",
    "    h24scaler = SCALER()\n",
    "    ###h24scaler.fit(np.array(range(0,24),dtype=\"float64\")[:, None])\n",
    "    \n",
    "    # Adding agregados escalados\n",
    "    ###for name, data in agregados:\n",
    "    ###    norm_data[name] = h24scaler.transform(data[:,None])\n",
    "    ###vprint(\"    Agregados escalados.\")\n",
    "    \n",
    "    # Adding Target\n",
    "    YLABELS = []\n",
    "    if SHIFT >= 0:\n",
    "        # Predecir el TARGET actual o en el pasado.\n",
    "        #norm_data[\"y\"] = norm_data[\"TARGET\"].shift(SHIFT)\n",
    "        #TARGET_TO_SHIFT = TARGET_TO_SHIFT\n",
    "        norm_data[\"y\"] = norm_data[\"y\"].shift(SHIFT)\n",
    "        YLABELS.append(\"y\")\n",
    "        norm_data = norm_data.drop(TARGET, axis=1)\n",
    "        print(\"    SHIFT==0        COLUMN %s removed from features\"%TARGET)\n",
    "        vprint(\"    y Target added.\")\n",
    "    #elif SHIFT == -1:\n",
    "    #    # PREDECIR EL SIGUIENTE DÍA\n",
    "    #    norm_data[\"y\"] = norm_data[TARGET].shift(SHIFT)\n",
    "    #    YLABELS.append(\"y\")\n",
    "    #    vprint(\"    y Target added.\")\n",
    "    else: #SHIFT < -1\n",
    "        # PREDECIR VARIOS DIAS HACIA ADELANTE\n",
    "        #norm_data[\"y\"] = norm_data[TARGET].shift(-1)\n",
    "        norm_data[\"y\"] = norm_data[\"y\"].shift(-1)\n",
    "        YLABELS.append(\"y\")\n",
    "        vprint(\"    y Target added.\")\n",
    "        M = SHIFT * -1\n",
    "        if PAST is False:\n",
    "            c = 1\n",
    "        else:\n",
    "            c = PAST*-1\n",
    "        while c <= M:\n",
    "            if c == 1:\n",
    "                c += 1\n",
    "                continue\n",
    "            #norm_data[\"y\"+str(c)] = norm_data[TARGET].shift(-1*c)\n",
    "            #TARGET_MASK = TARGET_MASK.shift(-1)\n",
    "            norm_data[\"y\"+str(c)] = norm_data[\"y\"].shift(-1*c)\n",
    "            #norm_data[\"y\"] = norm_data[\"y\"].where(TARGET_MASK)\n",
    "            YLABELS.append(\"y\"+str(c))\n",
    "            vprint(\"    y%d Target added.\"%(c))\n",
    "            c += 1\n",
    "    #print(norm_data[\"y\"].copy())\n",
    "    #return\n",
    "    \n",
    "    #norm_data = norm_data.fillna(method=\"ffill\")\n",
    "    \n",
    "    if FILTER_YEARS:\n",
    "        i,f = FILTER_YEARS\n",
    "        norm_data = norm_data[\"%d-11-01\"%i:\"%d-03-31\"%(f+1)]\n",
    "        #print(norm_data)\n",
    "    dataset = norm_data\n",
    "    \n",
    "    dataset = dataset[ (dataset.index.month>=11) |  (dataset.index.month<=3) ]\n",
    "    \n",
    "    #dataset[\"O3\"] = dataset[\"O3\"][\"2004-11-01\":\"2004-11-05\"]\n",
    "    \n",
    "    \n",
    "    #print(dataset)\n",
    "    \n",
    "    return dataset, YLABELS, Yscaler, h24scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alias for test\n",
    "#STATION, FILTER_YEARS, THETA = \"POH_full\", [2010, 2017], 56\n",
    "#STATION, FILTER_YEARS, THETA = \"Las_Condes\", [2004, 2013], 89\n",
    "#STATION, FILTER_YEARS, THETA = \"Parque_OHiggins\", [2009, 2017], 60\n",
    "STATION, FILTER_YEARS, THETA = get_station(\"Las_Condes\")\n",
    "tempConfig = {\n",
    "    \"STATION\": STATION,\n",
    "    \"SCALER\" : preprocessing.StandardScaler,\n",
    "    \"AGREGADOS\": [],#[\"ALL\"],\n",
    "    \"PRECALC\": [], #precalcular_agregados(STATION),\n",
    "    \"IMPUTATION\": None,\n",
    "    \"THETA\": THETA,\n",
    "    \"TARGET\":\"O3\",\n",
    "    \"SHIFT\":-1,\n",
    "    \"PAST\":False,\n",
    "    \"FILTER_YEARS\": FILTER_YEARS,\n",
    "}\n",
    "norm_data, YLABELS, dataScaler, __scaler = import_merge_and_scale(tempConfig, SCALE = False)\n",
    "original = norm_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## select_features\n",
    "Crea un DataFrame que contiene sólo los atributos indicados en FEATURES o los datos que contengan un porcentaje de no nulos mayor al CUT. Cuando se utiliza el CUT, se pueden banear atributos manualmente.  \n",
    "El DataFrame devuelto contiene todas filas en donde ningún dato es no nulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección de atributos en base a la cantidad de ejemplos sin datos nulos que se dispondrán\n",
    "def select_features(internalConfig, Config, verbose=True):\n",
    "    dataset = internalConfig['complete_dataset']\n",
    "    ylabels = internalConfig['ylabels']\n",
    "    FEATURES = Config['FIXED_FEATURES']\n",
    "    CUT = Config['CUT']\n",
    "    BAN = Config['BAN']\n",
    "    SHIFT = Config['SHIFT']\n",
    "    \n",
    "    vprint = print if verbose else lambda *a, **k: None\n",
    "    \n",
    "    print(\"Selecting features ...\")\n",
    "    if FEATURES == []:\n",
    "        vprint(\"    Using CUT=%.2f\"%CUT)\n",
    "        \n",
    "        a=dataset.isna().sum()\n",
    "        b=dataset.describe().iloc[0]\n",
    "        cantidad = (b/(a+b))\n",
    "        \n",
    "        atributos = cantidad[cantidad >= CUT]\n",
    "        excluidos = cantidad[cantidad < CUT]\n",
    "\n",
    "        index = atributos.index.values.tolist()\n",
    "        banned = []\n",
    "        for b in BAN:\n",
    "            if b in index:\n",
    "                index.remove(b)\n",
    "                banned.append(b)\n",
    "            if \"hm\"+b in index:\n",
    "                index.remove(\"hm\"+b)\n",
    "                banned.append(\"hm\"+b)\n",
    "        \n",
    "        vprint(\"    %i Atributos Excluidos:\"%(len(excluidos)), excluidos.index.values)\n",
    "        vprint(\"    %i Atributos Baneados:\"%(len(banned)),banned)\n",
    "    else:\n",
    "        vprint(\"    Using fixed features ...\")\n",
    "        if \"y\" in FEATURES:\n",
    "            print(\"    WARNING: 'y' no debe estar en los ATRIBUTOS, se removerá y agregara al final del dataset\")\n",
    "            FEATURES.remove(\"y\")\n",
    "            \n",
    "            \n",
    "        index = FEATURES + ylabels\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    data = dataset[index]\n",
    "    \n",
    "    #print(data.dropna(how=\"all\"))\n",
    "    #temp = data.dropna(how=\"all\")\n",
    "    #temp = temp.drop(\"y\",axis=1)\n",
    "    #temp.to_csv(\"data_con_nan.csv\",na_rep=\"NaN\")\n",
    "    \n",
    "    \n",
    "    #Removiendo ejemplos con al menos un atributo NaN\n",
    "    data.dropna(inplace=True)\n",
    "    \n",
    "    FEATURES = index\n",
    "    for y in ylabels:\n",
    "        FEATURES.remove(y)\n",
    "    vprint(\"    %i Atributos Seleccionadios:\"%(len(FEATURES)),FEATURES)\n",
    "    vprint(\"    Cantidad de dias totales disponibles:\",len(data))\n",
    "\n",
    "    \n",
    "    \n",
    "    return data, FEATURES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nonancheck\n",
    "Función de control para asegurarse que no existan datos no nulos.  \n",
    "La funcion es estatica por lo que los valores deben cambiarse dentro de la función si es que se desea probar algo más."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asegurandose que no hayan datos faltantes\n",
    "def nonancheck():\n",
    "    STATION = \"Las_Condes\"\n",
    "    tempConfig = {\n",
    "        \"STATION\" : STATION,\n",
    "        \"SCALER\" : preprocessing.StandardScaler,\n",
    "        \"IMPUTATION\":None,\n",
    "        \"FILTER_YEARS\" : [],\n",
    "        \"AGREGADOS\":[], \"PRECALC\":precalcular_agregados(STATION),\n",
    "        \"THETA\":89,\n",
    "        \"TARGET\":\"O3\",\n",
    "        \"SHIFT\":-1,\n",
    "        \"PAST\":False,\n",
    "        \"FIXED_FEATURES\":[],\n",
    "        \"CUT\": 0.28,\n",
    "        \"BAN\": [],\n",
    "        \n",
    "    }\n",
    "    tempIC={}\n",
    "    tempIC[\"complete_dataset\"], tempIC[\"ylabels\"], __Yscaler, __h24scaler = import_merge_and_scale(tempConfig, verbose=False)\n",
    "    data, __ = select_features(tempIC, tempConfig, verbose=False)\n",
    "    return data.isna().describe()\n",
    "\n",
    "nonancheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRUEBAS - CONTROL DE DATOS\n",
    "def y_vs_target():\n",
    "    STATION = \"Las_Condes\"\n",
    "    tempConfig = {\n",
    "        \"STATION\" : STATION,\n",
    "        \"SCALER\" : preprocessing.StandardScaler,\n",
    "        \"IMPUTATION\":None,\n",
    "        \"FILTER_YEARS\" : [],\n",
    "        \"AGREGADOS\":[], \"PRECALC\":precalcular_agregados(STATION),\n",
    "        \"THETA\":89,\n",
    "        \"TARGET\":\"O3\",\n",
    "        \"SHIFT\":-1,\n",
    "        \"PAST\":False,\n",
    "        \"FIXED_FEATURES\":[],\n",
    "        \"CUT\": 0.28,\n",
    "        \"BAN\": [],\n",
    "        \"GRAPH\":True,\n",
    "    }\n",
    "    tempIC={}\n",
    "    tempIC[\"complete_dataset\"], tempIC[\"ylabels\"], __Yscaler, __h24scaler = import_merge_and_scale(tempConfig, verbose=False)\n",
    "    data, __ = select_features(tempIC, tempConfig, verbose=False)\n",
    "    \n",
    "    dataset = tempIC[\"complete_dataset\"]\n",
    "    TARGET = tempConfig[\"TARGET\"]\n",
    "\n",
    "    oo = data[TARGET]\n",
    "    yy = dataset[\"y\"]\n",
    "\n",
    "    gg = pd.concat([yy,oo], axis=1)\n",
    "\n",
    "    gg.iplot()\n",
    "\n",
    "if for_graph():\n",
    "    y_vs_target()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## obtener_secuencias\n",
    "Obtiene las secuencias de distintos largos máximos posibles.  \n",
    "Devuelve un diccionario donde la llave es el largo de las secuencia y el valor es una lista con tuplas con la fecha inicial y final correspondiente a dicha secuencia de ese largo.  \n",
    "Ej. { 1 : [ ('2000-01-01', '2000-01-01') ], 3 : [ ('2006-01-01', '2006-01-03'), ('2006-02-03', '2006-02-05') ]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenemos la cantidad se secuencias de distinto largo maximo que se pueden hacer con los días correlativos.\n",
    "def obtener_secuencias(internalConfig):\n",
    "    data = internalConfig[\"data\"]\n",
    "    \n",
    "    secuencias = {}\n",
    "    fecha = data.index[0] \n",
    "    fin = data.index[-1] + np.timedelta64(1,\"D\")\n",
    "    largo = 0\n",
    "    i = 0\n",
    "    sec_i = fecha\n",
    "    while True:\n",
    "        i += 1\n",
    "        siguiente = fecha + np.timedelta64(1,\"D\")\n",
    "        if siguiente in data.index:\n",
    "            fecha = siguiente\n",
    "        else:\n",
    "            if i not in secuencias:\n",
    "                secuencias[i] = []\n",
    "            secuencias[i].append( (sec_i,fecha) )\n",
    "            i = 0\n",
    "            f_index = data.index.get_loc(fecha) + 1\n",
    "            if f_index != len(data.index):\n",
    "                fecha = data.index[f_index]\n",
    "                sec_i = fecha\n",
    "            else:\n",
    "                break\n",
    "    return secuencias\n",
    "        \n",
    "#SECUENCIAS = secuencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make_examples\n",
    "Recibe el dataset, las secuencias, la cantidad de TIMESTEP y crea los ejemplos con su correspondiente etiqueta.\n",
    "\n",
    "Por ejemplo, dado un TIMESTEP de 3.  \n",
    "Cada ejemplo tendrá 3 días sucesivos y una etiqueta Y asociada al ozono del día siguiente.  \n",
    "    Es decir, se utilizaran los datos en el tiempo T, T-1 y T-2 para predecir el ozono en tiempo T+1:  \n",
    "    (T-2),(T-1),(T)     ->   (T+1)  \n",
    "\n",
    "Desde el punto de vista del dataset, corresponde a 3 filas sucesivas en orden cronológico donde la etiqueta corresponde al valor Y de la última fila.\n",
    "\n",
    "La función devuelve:  \n",
    "> Un array 'examples' con los ejemplos de entrenamiento con el shape (cantidad, TIMESTEP, FEATURES).  \n",
    "> Un array 'y' con los Y en un arreglo bidimensional de sólo una columna con el shape (cantidad, 1).  \n",
    "> Un array 'date_index' con las fechas asociadas la PREDICCIÓN Y.  \n",
    "\n",
    "Los tres array estan ordenados temporalmente por lo que la date_index[i] corresponde a la etiqueta y[i] de los ejemplos examples[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Haremos los distintos ejemplos segun el TIMESTEP indicado.\n",
    "#NOTA: La fecha adjunta en cada ejemplo y lista 'y' corresponde a la fecha de la predicción 'y'\n",
    "#      y no a fecha de los datos, esto para poder graficar de forma ordenada despúes.\n",
    "#      Es decir, para predecir el ozono en un día X, tomo los datos de los TIMESTEP dias anteriores.\n",
    "def make_examples(internalConfig, Config, verbose=True):\n",
    "    data = internalConfig[\"data\"]\n",
    "    secuencias = internalConfig[\"secuencias\"]\n",
    "    ylabels = internalConfig[\"ylabels\"]\n",
    "    TIMESTEP = Config[\"TIMESTEP\"]\n",
    "    OVERLAP = Config[\"OVERLAP\"]\n",
    "    \n",
    "    \n",
    "    vprint = print if verbose else lambda *a, **k: None\n",
    "    \n",
    "    print(\"Making examples ...\")\n",
    "    #TIMESTEP= 3\n",
    "    #OVERLAP = True\n",
    "    \n",
    "    largos = list(secuencias.keys())\n",
    "    largos.sort()\n",
    "    examples = []\n",
    "    y = []\n",
    "    ylen = len(ylabels)\n",
    "    \n",
    "    for l in largos:\n",
    "        #print(l)\n",
    "        if l < TIMESTEP:\n",
    "            continue\n",
    "        for sec in secuencias[l]:\n",
    "            inicio, fin = sec\n",
    "            #print(sec)\n",
    "            s = data[inicio:fin].values\n",
    "            if OVERLAP:\n",
    "                i = 0\n",
    "                while i <= len(s) - TIMESTEP:\n",
    "                    c = 0\n",
    "                    new_example = []\n",
    "                    yy = []\n",
    "                    while c < TIMESTEP:\n",
    "                        new_example.append( s[i+c][:-ylen] )\n",
    "                        #TimeDistributed\n",
    "                        yy.append( s[i+c][-ylen:] )\n",
    "                        c+=1\n",
    "                    fecha = inicio + np.timedelta64(i+c, \"D\")\n",
    "                    ##new_example.reverse()\n",
    "                    examples.append( [fecha] + new_example  )\n",
    "                    ##y.append( ( fecha, s[i+c-1][-1] ) )\n",
    "                    ##y.append( [ fecha] + s[i+c-1][-ylen:].tolist() )\n",
    "                    #TimeDistributed\n",
    "                    y.append(( [fecha] + yy ))\n",
    "                    i += 1\n",
    "            else:\n",
    "                i = 0\n",
    "                while i <= len(s) - TIMESTEP:\n",
    "                    c = 0\n",
    "                    new_example = []\n",
    "                    #yy=[]\n",
    "                    while c < TIMESTEP:\n",
    "                        new_example.append( s[i][:-1] )\n",
    "                        #yy.append( s[i][-1] )\n",
    "                        c += 1\n",
    "                        i += 1\n",
    "                    fecha = inicio + np.timedelta64(i, \"D\")\n",
    "                    ##new_example.reverse()\n",
    "                    examples.append( [fecha] + new_example  )\n",
    "                    y.append( ( fecha, s[i-1][-1] ) )\n",
    "                    #y.append(( [fecha] + yy ))\n",
    "            #break\n",
    "        #break\n",
    "\n",
    "        \n",
    "    print(\"    Ejemplos Disponibles: , \",len(examples))\n",
    "    vprint(\"    len(y), \",len(y))\n",
    "\n",
    "    \n",
    "    # Sort by Date of prediction Y\n",
    "    examples.sort()\n",
    "    y.sort()\n",
    "    \n",
    "   \n",
    "    examples = np.array(examples)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Get date index of all examples in order. To use it later\n",
    "    date_index = y[:,0]\n",
    "    vprint(\"    len(date_index), \", len(date_index))\n",
    "    \n",
    "    \n",
    "    examples = np.array( examples[:,1:].tolist() )\n",
    "    y = np.array( y[:,1:].tolist() )\n",
    "    #y = y[:,1:]\n",
    "    \n",
    "    vprint(\"    examples.shape :\", examples.shape )\n",
    "    vprint(\"    y.shape :\", y.shape  )\n",
    "    return examples, y, date_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## join_index\n",
    "Función auxiliar que se utiliza para indexar un columna de fechas a una columna de datos, y así poder asociarlas temporalmente junto con otras columnas o datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Une la fecha a un arreglo de valores, ambos de igual largo\n",
    "# Retorna un DataFrame\n",
    "def join_index(date_index, array, label):\n",
    "    date_index = date_index.reshape( (-1,1) )\n",
    "    array = array.reshape( (-1,1) )\n",
    "    df = pd.DataFrame(np.hstack([date_index, array]))\n",
    "    df.columns = [\"fecha\",label]\n",
    "    df = df.set_index(\"fecha\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot_y_true\n",
    "Función estática y de control.  \n",
    "Para todos los ejemplos, grafica el valor de Y y la característica que representa en los distintos TIMESTEP.  \n",
    "Para el ejemplo, grafica el Ozono como TARGET con un TIMESTEP de 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot del TARGET y su valor actual\n",
    "def plot_y_true():\n",
    "    #STATION = \"Las_Condes\"\n",
    "    STATION, FILTER_YEARS, THETA = get_station(\"Las_Condes\")\n",
    "    tempConfig = {\n",
    "        \"STATION\":STATION,\n",
    "        \"SCALER\" : preprocessing.StandardScaler,\n",
    "        \"IMPUTATION\":None,\n",
    "        \"FILTER_YEARS\" : FILTER_YEARS,\n",
    "        \"AGREGADOS\":[],\n",
    "        \"PRECALC\":precalcular_agregados(STATION),\n",
    "        \"THETA\":THETA,\n",
    "        \"TARGET\":\"O3\",\n",
    "        \"TIMESTEP\":5,\n",
    "        \"OVERLAP\":True,\n",
    "        \"SHIFT\":-1,\n",
    "        \"PAST\":False,\n",
    "        \"FIXED_FEATURES\":['CO', 'PM10', 'PM25', 'NO', 'NOX', 'WD', 'RH', 'TEMP', 'WS', 'UVA', 'UVB', 'O3'],\n",
    "        \"CUT\": 0.41,\n",
    "        \"BAN\": [\"EC\",\"countEC\", 'O3btTHETA'],\n",
    "        \"GRAPH\": True\n",
    "    }\n",
    "    tempIC={}\n",
    "    tempIC[\"complete_dataset\"], tempIC[\"ylabels\"], __Yscaler, __h24scaler = import_merge_and_scale(tempConfig, verbose=False, SCALE=False)\n",
    "    tempIC[\"data\"], tempIC[\"features\"] = select_features(tempIC, tempConfig, verbose=True)\n",
    "    tempIC[\"secuencias\"] = obtener_secuencias(tempIC)\n",
    "    examples, y, date_index = make_examples(tempIC, tempConfig, verbose=False)\n",
    "    \n",
    "    FEATURES = tempIC[\"features\"]\n",
    "    TARGET = tempConfig[\"TARGET\"]\n",
    "    TIMESTEP = tempConfig[\"TIMESTEP\"]\n",
    "    \n",
    "    f = FEATURES.index(TARGET)\n",
    "    \n",
    "    print(y.shape)\n",
    "\n",
    "    df = join_index(date_index, y[:,-1,0], \"y (T+1)\")\n",
    "    ### El indice esta mostrando el ozono, es un array, no un dataframe\n",
    "    \n",
    "    for i in range(0, TIMESTEP):\n",
    "        label = \"T\" if i==0 else \"T-%d\"%i\n",
    "        gf = join_index(date_index, examples[:,TIMESTEP-1-i,f], label)\n",
    "        df = pd.concat([df,gf], axis=1)\n",
    "    df.asfreq(\"D\").iplot(title=\"TARGET: %s, index: %d\"%(TARGET,f), hline=[THETA])\n",
    "\n",
    "    #df[np.datetime64(\"2014-01-03\"):np.datetime64(\"2014-01-08\")]\n",
    "    \n",
    "    \n",
    "if for_graph():\n",
    "    plot_y_true()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make_traintest\n",
    "Divide los array 'examples', 'y' y 'data_index', según un porcentaje dado como trainset y testset.  \n",
    "Además recorta el trainset y testset para hacerlos divisibles por el BATCH_SIZE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split train and test datasets\n",
    "def make_traintest(internalConfig, Config, verbose=True):\n",
    "    examples = internalConfig[\"examples\"]\n",
    "    y = internalConfig[\"y_examples\"]\n",
    "    date_index = internalConfig[\"dateExamples\"]\n",
    "    y_len = internalConfig[\"y_len\"]\n",
    "    BATCH_SIZE = Config[\"BATCH_SIZE\"]\n",
    "    TRAINPCT = Config[\"TRAINPCT\"]\n",
    "    TIMEDIST = Config[\"TIMEDIST\"]\n",
    "    TIMESTEP = Config[\"TIMESTEP\"]\n",
    "    SHUFFLE = Config[\"SHUFFLE\"]\n",
    "    \n",
    "    \n",
    "    vprint = print if verbose else lambda *a, **k: None\n",
    "    \n",
    "    print(\"Making Trainset y Testset ...\")\n",
    "    train_size = int( len(examples)*TRAINPCT )\n",
    "    test_size = len(examples) - train_size\n",
    "    \n",
    "    ### shuffle examples\n",
    "    if SHUFFLE:\n",
    "        np.random.seed(123)\n",
    "        shuffle_index = list(range(len(examples)))\n",
    "        np.random.shuffle(shuffle_index)\n",
    "        examples = examples[shuffle_index]\n",
    "        date_index = date_index[shuffle_index]\n",
    "        y = y[shuffle_index]\n",
    "    \n",
    "    np.random.seed(123)  \n",
    "    \n",
    "    \n",
    "    trainX, trainY = examples[0:train_size], y[0:train_size]\n",
    "    dateTrain = date_index[0:train_size]\n",
    "    \n",
    "    testX, testY = examples[train_size:], y[train_size:]\n",
    "    dateTest = date_index[train_size:]\n",
    "    \n",
    "    \n",
    "    ###Cortar los dataset para hacerlos calzar con el BATCH_SIZE, necesario cuando se hace stateful\n",
    "    ##vprint(\"    len of Trainset before clip: \", len(trainX))\n",
    "    ##vprint(\"    len of Testset before clip: \", len(testX))\n",
    "    ##\n",
    "    ##cuttr = int(train_size/BATCH_SIZE)*BATCH_SIZE\n",
    "    ##trainX = trainX[:cuttr]\n",
    "    ##trainY = trainY[:cuttr]\n",
    "    ##dateTrain = dateTrain[:cuttr]\n",
    "    ##print(\"        Discargind %d last examples in Trainset to match with batch size of %d\"%(train_size-cuttr, BATCH_SIZE))\n",
    "    ##\n",
    "    ##cuttst = int(test_size/BATCH_SIZE)*BATCH_SIZE\n",
    "    ##testX = testX[:cuttst]\n",
    "    ##testY = testY[:cuttst]\n",
    "    ##dateTest = dateTest[:cuttst]\n",
    "    ##print(\"        Discargind %d last examples in Testset to match with batch size of %d\"%(test_size-cuttst, BATCH_SIZE))\n",
    "    \n",
    "    ###\n",
    "    ### VALIDATION SPLIT\n",
    "    ###\n",
    "    train_size = int( len(trainX)*0.85 )\n",
    "    validation_size = len(trainX) - train_size\n",
    "    print(len(trainX),train_size,validation_size)\n",
    "    \n",
    "    tempX = trainX\n",
    "    tempY = trainY\n",
    "    tempDate = dateTrain\n",
    "    \n",
    "    trainX, trainY = tempX[0:train_size], tempY[0:train_size]\n",
    "    dateTrain = tempDate[0:train_size]\n",
    "    \n",
    "    validX, validY = tempX[train_size:], tempY[train_size:]\n",
    "    dateValid = tempDate[train_size:]\n",
    "    \n",
    "    #print(\"trainY.shape, \",trainY.shape)\n",
    "    #print(\"validX.shape,\", validX.shape)\n",
    "    #print(\"validY.shape, \",validY.shape)\n",
    "    \n",
    "    if TIMEDIST == True:\n",
    "        trainY = trainY.reshape(-1, TIMESTEP, y_len )\n",
    "        validY = validY.reshape(-1, TIMESTEP, y_len )\n",
    "        testY  = testY.reshape( -1, TIMESTEP, y_len )\n",
    "    else:\n",
    "        trainY = trainY[:,-1]\n",
    "        validY = validY[:,-1]\n",
    "        testY  = testY[:,-1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    vprint(\"    trainX.shape, \", trainX.shape)\n",
    "    vprint(\"    validX.shape, \", validX.shape)\n",
    "    vprint(\"    testX.shape , \", testX.shape)\n",
    "    \n",
    "    vprint(\"    trainY.shape, \", trainY.shape)\n",
    "    vprint(\"    validY.shape, \", validY.shape)\n",
    "    vprint(\"    testY.shape, \", testY.shape)\n",
    "    \n",
    "    d = {\n",
    "        'trainX':      trainX,\n",
    "        'trainY':      trainY,\n",
    "        'validX':      validX,\n",
    "        'validY':      validY,\n",
    "        'testX' :      testX,\n",
    "        'testY' :      testY,\n",
    "        'dateTrain'  : dateTrain,\n",
    "        'dateValid'  : dateValid,\n",
    "        'dateTest'   : dateTest\n",
    "    }\n",
    "    \n",
    "    return d\n",
    "    #return trainX, trainY, validX, validY, testX, testY, dateTrain, dateValid, dateTest\n",
    "\n",
    "\n",
    "#join_index(dateTrain, trainY,\"train\").iplot()\n",
    "#join_index(dateTest, testY,\"test\").iplot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make_folds_TVT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_folds_TVT(internalConfig, Config):\n",
    "    examples = internalConfig[\"examples\"]\n",
    "    y_examples = internalConfig[\"y_examples\"]\n",
    "    dateExamples = internalConfig[\"dateExamples\"]\n",
    "    LAGS = Config[\"TIMESTEP\"]\n",
    "    STARTYEAR, ENDYEAR = Config[\"FILTER_YEARS\"]\n",
    "    \n",
    "    list_trainX = []\n",
    "    list_trainY = []\n",
    "    list_validX = []\n",
    "    list_validY = []\n",
    "    list_testX = []\n",
    "    list_testY = []\n",
    "    list_dateTrain = []\n",
    "    list_dateValid = []\n",
    "    list_dateTest = []\n",
    "    \n",
    "    inicio = \"{}-11-01\".format\n",
    "    fin = \"{}-03-31\".format\n",
    "    dates = dateExamples.astype(\"datetime64\")\n",
    "    firstyear = int(str(dateExamples[0])[0:4])\n",
    "    firstyear = STARTYEAR\n",
    "    for year in range(firstyear, ENDYEAR-1):\n",
    "        trainRange = [ np.datetime64(inicio(firstyear)), np.datetime64(fin(year+1)) ]\n",
    "        validRange = [ np.datetime64(inicio(year+1))   , np.datetime64(fin(year+2)) ]\n",
    "        testRange =  [ np.datetime64(inicio(year+2))   , np.datetime64(fin(year+3)) ]\n",
    "        #print(year)\n",
    "        #print(trainRange)\n",
    "        #print(validRange)\n",
    "        #print(testRange)\n",
    "        \n",
    "        mask = (trainRange[0] <= dates) & (dates <= trainRange[1])\n",
    "        trainX = examples[mask]\n",
    "        trainY = y_examples[mask][:,-1]\n",
    "        dateTrain = dateExamples[mask]\n",
    "        #print(mask)\n",
    "        \n",
    "        mask = (validRange[0] <= dates) & (dates <= validRange[1])\n",
    "        validX = examples[mask]\n",
    "        validY = y_examples[mask][:,-1]\n",
    "        dateValid = dateExamples[mask]\n",
    "        #print(mask)\n",
    "        \n",
    "        mask = (testRange[0] <= dates) & (dates <= testRange[1])\n",
    "        testX = examples[mask]\n",
    "        testY = y_examples[mask][:,-1]\n",
    "        dateTest = dateExamples[mask]\n",
    "        #print(mask)\n",
    "        \n",
    "        if len(trainX) != 0 and len(validX) != 0 and len(testX) != 0:\n",
    "            list_trainX.append(trainX)\n",
    "            list_trainY.append(trainY)\n",
    "            list_validX.append(validX)\n",
    "            list_validY.append(validY)\n",
    "            list_testX.append(testX)\n",
    "            list_testY.append(testY)\n",
    "            list_dateTrain.append(dateTrain)\n",
    "            list_dateValid.append(dateValid)\n",
    "            list_dateTest.append(dateTest)\n",
    "            print(\"Usando {}-{}, lags: {}\".format(firstyear,year+2,LAGS), (len(trainX),len(validX),len(testX)) )\n",
    "        else:\n",
    "            print(\"{}-{} excluido, lags: {}, examples:\".format(firstyear,year+2,LAGS), (len(trainX),len(validX),len(testX)) )\n",
    "    \n",
    "    d = {\n",
    "        'list_trainX':list_trainX,\n",
    "        'list_trainY':list_trainY,\n",
    "        'list_validX':list_validX,\n",
    "        'list_validY':list_validY,\n",
    "        'list_testX' :list_testX,\n",
    "        'list_testY' :list_testY,\n",
    "        'list_dateTrain'  :list_dateTrain,\n",
    "        'list_dateValid'  :list_dateValid,\n",
    "        'list_dateTest'   :list_dateTest\n",
    "    }\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_filename(internalConfig, Config):\n",
    "    global MODELS_FOLDER\n",
    "    if \"subFeatures\" in Config:\n",
    "        features = Config[\"subFeatures\"]\n",
    "        f_len = len(features)\n",
    "    else:\n",
    "        features = internalConfig[\"features\"]\n",
    "        f_len = internalConfig[\"f_len\"]\n",
    "    THETA = Config[\"THETA\"]\n",
    "    FUTURE = Config[\"FUTURE\"]\n",
    "    PAST = Config[\"PAST\"]\n",
    "    TARGET = Config[\"TARGET\"]\n",
    "    BATCH_SIZE = Config[\"BATCH_SIZE\"]\n",
    "    STATION = Config[\"STATION\"]\n",
    "    SEED = Config[\"SEED\"]\n",
    "    if \"subTimeStep\" in Config:\n",
    "        TIMESTEP = Config[\"subTimeStep\"]\n",
    "    else:\n",
    "        TIMESTEP = Config[\"TIMESTEP\"]\n",
    "    TRAINPCT = Config[\"TRAINPCT\"]\n",
    "    #OVERWRITE_MODEL = Config[\"OVERWRITE_MODEL\"]\n",
    "    MODEL_NAME = Config[\"MODEL_NAME\"]\n",
    "    LAYERS = Config[\"LAYERS\"]\n",
    "    EPOCHS = Config[\"EPOCHS\"]\n",
    "    DROP_RATE = Config[\"DROP_RATE\"]\n",
    "    TIMEDIST = Config[\"TIMEDIST\"]\n",
    "    SHUFFLE = Config[\"SHUFFLE\"]\n",
    "    if \"QUANTILES\" in Config:\n",
    "        strQUANTILES = \"-\" + str(Config[\"QUANTILES\"]).replace(\"0.\",\".\")\n",
    "    else:\n",
    "        strQUANTILES = \"\"\n",
    "        \n",
    "    if \"isLSTM\" in Config:\n",
    "        isLSTM = \"-\"+str(Config[\"isLSTM\"])\n",
    "    else:\n",
    "        isLSTM = \"\"\n",
    "    \n",
    "    if \"qfileHash\" in internalConfig:\n",
    "        qfileHash = \"-\"+str(internalConfig[\"qfileHash\"])\n",
    "    else:\n",
    "        qfileHash = \"\"\n",
    "    if \"LOSS\" in Config:\n",
    "        MODEL_NAME += Config[\"LOSS\"]\n",
    "    \n",
    "    if Config[\"FOLDS_TVT\"]:\n",
    "        foldmax = len(internalConfig[\"list_trainX\"])\n",
    "        currentFold = internalConfig[\"fold\"]\n",
    "        TVT = \"-TVT%sof%s\"%(currentFold,foldmax)\n",
    "    else:\n",
    "        TVT = \"\"\n",
    "    \n",
    "    if Config[\"FILTER_YEARS\"]:\n",
    "        i,f = Config[\"FILTER_YEARS\"]\n",
    "        fyears = \"-%d-%d\"%(i,f)\n",
    "    else:\n",
    "        fyears = \"\"\n",
    "        \n",
    "    IMPUT = Config[\"IMPUTATION\"]\n",
    "    \n",
    "    strFEATURES = str(features).replace(\"'\",\"\")\n",
    "    strTD = \"TD\"*TIMEDIST\n",
    "    FILE_NAME = \"./%s/%s-%.2f-(%d,%d,%d)%s-%s-%.3f-%s-%s-%d-(%s,%s)-%s-%s%sSHU%s%s%s%s%s%s%s.h5\"%(MODELS_FOLDER, MODEL_NAME, THETA, BATCH_SIZE, TIMESTEP, f_len, strQUANTILES, strFEATURES, TRAINPCT, TARGET, LAYERS, EPOCHS, str(PAST), FUTURE, strTD, DROP_RATE, isLSTM , SHUFFLE,TVT, fyears,STATION,SEED,IMPUT, qfileHash )\n",
    "    FILE_NAME = FILE_NAME.replace(\" \",\"\")\n",
    "    \n",
    "    return FILE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE y MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(ytrue, ypred, THETA=False, norm=False, ALL=False):\n",
    "    ytrue = ytrue.reshape(-1)\n",
    "    ypred = ypred.reshape(-1)\n",
    "    if ALL == True:\n",
    "        return {\"RMSE\"    : RMSE(ytrue, ypred),\n",
    "                \"RMSPE\"   : RMSE(ytrue, ypred, norm=True),\n",
    "                \"RMSEat\"  : RMSE(ytrue, ypred, THETA=THETA),\n",
    "                \"RMSPEat\" : RMSE(ytrue, ypred, THETA=THETA, norm=True)\n",
    "               }\n",
    "    \n",
    "    if THETA is False:\n",
    "        includes = np.ones(len(ytrue), dtype=bool)\n",
    "    else:\n",
    "        includes = ytrue >= THETA\n",
    "    \n",
    "    if includes.any():\n",
    "        yt = ytrue[includes]\n",
    "        yp = ypred[includes]\n",
    "        e = yt - yp\n",
    "        if norm == True:\n",
    "            e = e/yt\n",
    "            #if THETA == False:\n",
    "            #    #for i in range(len(e)):\n",
    "            #    #    print(yt[i],yp[i],yt[i]-yp[i], np.abs(e[i]))\n",
    "            #    print(np.mean(np.abs(e)))\n",
    "        #print(\"###\")\n",
    "        e2 = e**2\n",
    "        \n",
    "        mean = np.mean(e2)\n",
    "        #if THETA == False and norm == True:\n",
    "        #    for i in range(len(e2)):\n",
    "        #        if (np.abs(e[i]) >= 1):\n",
    "        #            print(yt[i],yp[i],yt[i]-yp[i], np.abs(e[i]),e2[i])\n",
    "        #    print(mean)\n",
    "        return np.sqrt(mean)\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE(ytrue, ypred, THETA=False, norm=False, ALL=False):\n",
    "    if ALL == True:\n",
    "        return {\"MAE\"    : MAE(ytrue, ypred),\n",
    "                \"MAPE\"   : MAE(ytrue, ypred, norm=True),\n",
    "                \"MAEat\"  : MAE(ytrue, ypred, THETA=THETA),\n",
    "                \"MAPEat\" : MAE(ytrue, ypred, THETA=THETA, norm=True)\n",
    "               }\n",
    "    \n",
    "    if THETA is False:\n",
    "        includes = np.ones(len(ytrue), dtype=bool)\n",
    "    else:\n",
    "        includes = ytrue >= THETA\n",
    "    \n",
    "    if includes.any():\n",
    "        yt = ytrue[includes]\n",
    "        yp = ypred[includes]\n",
    "        e = yt - yp\n",
    "        if norm == True:\n",
    "            e = e/yt\n",
    "        eabs = np.abs(e)\n",
    "        mean = np.mean(eabs)\n",
    "        return mean\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantile Metrics & Interval Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ytrue = actual\n",
    "#ypred = quantile_values\n",
    "#quantile_probs -> real quantiles\n",
    "def quantile_metrics(actual, quantile_probs, quantile_values):\n",
    "    actual = actual.reshape(-1)\n",
    "    quantile_losses = []\n",
    "    \n",
    "    for idx in range(len(quantile_probs)):\n",
    "        #print(actual.shape)\n",
    "        #print(quantile_values.shape)\n",
    "        #print(quantile_values[:,idx].shape)\n",
    "        res = actual - quantile_values[:,idx]\n",
    "        #print(res.shape)\n",
    "        q = quantile_probs[idx]\n",
    "        t=np.maximum(q*res, (q-1.0)*res)\n",
    "        #print(t.shape)\n",
    "        #print(\"######\")\n",
    "        \n",
    "        qloss = np.mean(t) #pinball loss\n",
    "        #if q == 0.9:\n",
    "        #    print(actual[-15:])\n",
    "        #    print(quantile_values[:,idx][-15:])\n",
    "        #    print(res[-15:])\n",
    "        #    print(t[-15:])\n",
    "        quantile_losses.append(qloss)\n",
    "\n",
    "    quantile_losses = np.array(quantile_losses)\n",
    "    return np.mean(quantile_losses), quantile_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IC_metrics(inf_limit, sup_limit, nominal_sig, true_values, base_prediction=0.0):\n",
    "    inf_limit = inf_limit.flatten()\n",
    "    sup_limit = sup_limit.flatten()\n",
    "    true_values = true_values.flatten()\n",
    "    base_prediction = base_prediction.flatten()\n",
    "    \n",
    "    EPS_CONST = np.finfo(float).eps\n",
    "    rho = nominal_sig\n",
    "    cov_prob = 0.0\n",
    "    \n",
    "    excess = []\n",
    "    \n",
    "    for idx in range(len(true_values)):\n",
    "            one_excess = 0.0\n",
    "            if true_values[idx] > sup_limit[idx]:\n",
    "                    one_excess = (2.0/rho)*(true_values[idx] - sup_limit[idx])\n",
    "            elif true_values[idx] < inf_limit[idx]:\n",
    "                    one_excess = (2.0/rho)*(inf_limit[idx] - true_values[idx])\n",
    "            else: \n",
    "                    cov_prob += 1.0\n",
    "\n",
    "            excess.append(one_excess)\n",
    "\n",
    "    excess = np.array(excess)\n",
    "    MIS = sup_limit - inf_limit + excess #mean interval score\n",
    "    MSIS =  np.divide(MIS, np.abs(true_values - base_prediction) + EPS_CONST) #mean scaled interval score\n",
    "    #MSIS =  np.divide(MIS, np.abs(true_values) + EPS_CONST) #mean scaled interval score\n",
    "    lenght = sup_limit - inf_limit\n",
    "\n",
    "    #return np.mean(MIS), np.mean(MSIS), np.mean(cov_prob), np.mean(lenght)\n",
    "    return np.mean(MIS), np.mean(MSIS), cov_prob/len(true_values), np.mean(lenght)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "LSTM con función de pérdida de 'mean_squared_error'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## myLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myLSTM(Config): #, AGREGADOS, TARGET, PRECALC, THETA, FUTURE, PAST, FEATURES, CUT, BAN, TIMESTEP, OVERLAP, BATCH_SIZE, TRAINPCT, OVERWRITE_MODEL, MODEL_NAME, LAYERS, EPOCHS, TIMEDIST, Yx):\n",
    "    #np.random.seed(123)\n",
    "    #set_random_seed(2)\n",
    "    \n",
    "    FOLDS_TVT = Config[\"FOLDS_TVT\"]\n",
    "    TIMESTEP = Config['TIMESTEP']\n",
    "    #TIMEDIST = Config['TIMEDIST']\n",
    "    Config['SHIFT'] = Config['FUTURE'] * -1\n",
    "    \n",
    "    #SHIFT = FUTURE*-1\n",
    "    #PRECALC = precalcular_agregados()\n",
    "    \n",
    "    \n",
    "    scalers = {}\n",
    "    ic = {\"scalers\": scalers}\n",
    "    \n",
    "    complete_dataset, ylabels, Yscaler, h24scaler = import_merge_and_scale(Config, verbose=False)\n",
    "    ic[\"complete_dataset\"] = complete_dataset\n",
    "    ic[\"ylabels\"] = ylabels\n",
    "    scalers['Yscaler'] = Yscaler\n",
    "    scalers[\"h24scaler\"] = h24scaler\n",
    "    \n",
    "    # TEST - Agregando datos del día siguiente - Son lineas COMENTABLES\n",
    "    #dataset = pd.concat([dataset[dataset.columns[:-1]], dataset[[\"TEMP\",\"UVB\"]].shift(-1), dataset[dataset.columns[-1:]]], axis=1)\n",
    "    #dataset.columns = dataset.columns[:-3].tolist() + [\"TEMP2\",\"UVB2\",\"y\"]\n",
    "    #dataset[[\"TEMP\",\"UVB\"]] = dataset[[\"TEMP\",\"UVB\"]].shift(-1)\n",
    "    \n",
    "    y_len = len(ic[\"ylabels\"])\n",
    "    ic[\"y_len\"] = y_len\n",
    "    \n",
    "    \n",
    "    data, features = select_features(ic, Config)\n",
    "    ic[\"data\"] = data\n",
    "    ic[\"features\"] = features\n",
    "    \n",
    "    ic[\"f_len\"] = len(ic[\"features\"])\n",
    "    \n",
    "    ic[\"secuencias\"] = obtener_secuencias(ic)\n",
    "    \n",
    "    #data, FEATURES = select_features(dataset, ylabels, FEATURES, CUT, BAN)\n",
    "    #F_LEN = len(FEATURES)\n",
    "    \n",
    "    #strFEATURES = str(FEATURES).replace(\"'\",\"\")\n",
    "    #strTD = \"TD\"*TIMEDIST\n",
    "    #FILE_NAME = \"./%s/%s-(%d,%d,%d)-%s-%.3f-%s-%s-%d-(%s,%s)-%s.h5\"%(MODELS_FOLDER,MODEL_NAME, BATCH_SIZE, TIMESTEP, F_LEN, strFEATURES, TRAINPCT, TARGET, LAYERS, EPOCHS, str(PAST), FUTURE, strTD)\n",
    "    #FILE_NAME = FILE_NAME.replace(\" \",\"\")\n",
    "    \n",
    "    \n",
    "    #SECUENCIAS = obtener_secuencias(data)\n",
    "    #examples, y, date_index = make_examples(data, SECUENCIAS, ylabels, TIMESTEP, OVERLAP, verbose=False)\n",
    "    #trainX, trainY, testX, testY, dateTrain, dateTest = make_traintest(examples, y, date_index, BATCH_SIZE, TRAINPCT, verbose=False)\n",
    "    \n",
    "    examples, y_examples, dateExamples = make_examples(ic, Config, verbose=False)\n",
    "    ic[\"examples\"] = examples\n",
    "    ic[\"y_examples\"] = y_examples\n",
    "    ic[\"dateExamples\"] = dateExamples\n",
    "    \n",
    "    if FOLDS_TVT == False:\n",
    "        tvtDict = make_traintest(ic, Config, verbose=False)\n",
    "        for key in tvtDict:\n",
    "            ic[key] = tvtDict[key]\n",
    "        #trainX, trainY, validX, validY, testX, testY, dateTrain, dateValid, dateTest = make_traintest(ic, Config, verbose=False)\n",
    "        #ic[\"trainX\"] = trainX\n",
    "        #ic[\"trainY\"] = trainY\n",
    "        #ic[\"validX\"] = validX\n",
    "        #ic[\"validY\"] = validY\n",
    "        #ic[\"testX\"] = testX\n",
    "        #ic[\"testY\"] = testY\n",
    "        #ic[\"dateTrain\"] = dateTrain\n",
    "        #ic[\"dateValid\"] = dateValid\n",
    "        #ic[\"dateTest\"] = dateTest\n",
    "    else:\n",
    "        list_tvtDict = make_folds_TVT(ic, Config)\n",
    "        for key in list_tvtDict:\n",
    "            ic[key] = list_tvtDict[key]\n",
    "    \n",
    "    list_models , file_name = myLSTMModel(ic, Config)\n",
    "    \n",
    "    if FOLDS_TVT == False:\n",
    "        ic[\"model\"] = list_models[0]\n",
    "        ic[\"list_models\"] = [ ic[\"model\"] ]\n",
    "    else:\n",
    "        ic[\"list_models\"] = list_models\n",
    "    \n",
    "    ic[\"file_name\"] = file_name #solo es el ultimo modelo\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    detail = myLSTMPredict(ic, Config)\n",
    "    ic[\"detail\"] = detail\n",
    "    #trainPred, validPred, testPred = myLSTMPredict(ic, Config)\n",
    "    #ic[\"trainPred\"] = trainPred\n",
    "    #ic[\"validPred\"] = validPred\n",
    "    #ic[\"testPred\"] = testPred\n",
    "    \n",
    "    return ic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## myLSTMPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myLSTMPredict(internalConfig, Config):\n",
    "    TIMEDIST = Config[\"TIMEDIST\"]\n",
    "    BATCH_SIZE = Config[\"BATCH_SIZE\"]\n",
    "    THETA = Config[\"THETA\"]\n",
    "    TARGET = Config[\"TARGET\"]\n",
    "    FOLDS_TVT = Config[\"FOLDS_TVT\"]\n",
    "    GRAPH = Config[\"GRAPH\"]\n",
    "    moreTHETA = Config[\"moreTHETA\"]\n",
    "    Yx = Config[\"Yx\"]\n",
    "    Yscaler = internalConfig[\"scalers\"][\"Yscaler\"]\n",
    "    list_models = internalConfig[\"list_models\"]\n",
    "    \n",
    "    if FOLDS_TVT == False:\n",
    "        list_trainX = [ internalConfig[   \"trainX\"] ]\n",
    "        list_trainY = [ internalConfig[   \"trainY\"] ]\n",
    "        list_validX = [ internalConfig[   \"validX\"] ]\n",
    "        list_validY = [ internalConfig[   \"validY\"] ]\n",
    "        list_testX  = [ internalConfig[   \"testX\" ] ]\n",
    "        list_testY  = [ internalConfig[   \"testY\" ] ]\n",
    "        list_dateTrain = [ internalConfig[\"dateTrain\"] ]\n",
    "        list_dateValid = [ internalConfig[\"dateValid\"] ]\n",
    "        list_dateTest  = [ internalConfig[ \"dateTest\"] ]\n",
    "        \n",
    "    else:\n",
    "        list_trainX = internalConfig[   \"list_trainX\"]\n",
    "        list_trainY = internalConfig[   \"list_trainY\"]\n",
    "        list_validX = internalConfig[   \"list_validX\"]\n",
    "        list_validY = internalConfig[   \"list_validY\"]\n",
    "        list_testX = internalConfig[    \"list_testX\"]\n",
    "        list_testY = internalConfig[    \"list_testY\"]\n",
    "        list_dateTrain = internalConfig[\"list_dateTrain\"]\n",
    "        list_dateValid = internalConfig[\"list_dateValid\"]\n",
    "        list_dateTest = internalConfig[ \"list_dateTest\"]\n",
    "    \n",
    "    \n",
    "    # RMSE\n",
    "    trainRMSE = []\n",
    "    validRMSE = []\n",
    "    testRMSE  = []\n",
    "    \n",
    "    # MAE\n",
    "    trainMAE = []\n",
    "    validMAE = []\n",
    "    testMAE  = []\n",
    "    \n",
    "    scores_detail = {\n",
    "                \"train\": {'quantity':[]},\n",
    "                \"valid\": {'quantity':[]},\n",
    "                \"test\" : {'quantity':[]},\n",
    "                }\n",
    "    \n",
    "    list_df = []\n",
    "    \n",
    "    print(\"Calculando Predicciones\")\n",
    "\n",
    "    for fold in range(0,len(list_trainX)):\n",
    "        trainX = list_trainX[fold]\n",
    "        trainY = list_trainY[fold]\n",
    "        validX = list_validX[fold]\n",
    "        validY = list_validY[fold]\n",
    "        testX  =  list_testX[fold]\n",
    "        testY  =  list_testY[fold]\n",
    "        dateTrain = list_dateTrain[fold]\n",
    "        dateValid = list_dateValid[fold]\n",
    "        dateTest  = list_dateTest[fold]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        classicModel = list_models[fold]\n",
    "    \n",
    "    \n",
    "        # Predictions\n",
    "        \n",
    "        trainPred = classicModel.predict(trainX)\n",
    "        classicModel.reset_states()\n",
    "        validPred = classicModel.predict(validX)\n",
    "        classicModel.reset_states()\n",
    "        testPred = classicModel.predict(testX)\n",
    "        classicModel.reset_states()\n",
    "        \n",
    "        #TimeDistributed\n",
    "        T = Yx\n",
    "        if TIMEDIST == True:\n",
    "            #print(trainPred.shape)\n",
    "            #print(trainY.shape)\n",
    "            trainPred = trainPred[:,-1, T].reshape(-1, 1)\n",
    "            trainY = trainY[:,-1,0].reshape(-1, 1)\n",
    "            validPred = validPred[:,-1, T].reshape(-1, 1)\n",
    "            validY = validY[:,-1,0].reshape(-1, 1)\n",
    "            testPred = testPred[:,-1, T].reshape(-1, 1)\n",
    "            testY = testY[:,-1,0].reshape(-1, 1)\n",
    "            #print(trainPred.shape)\n",
    "            #print(trainPred.shape)\n",
    "        else:\n",
    "            #print(trainPred.shape)\n",
    "            #print(trainY.shape)\n",
    "            trainPred = trainPred[:, T, None]\n",
    "            trainY = trainY[:, T, None]\n",
    "            validPred = validPred[:, T, None]\n",
    "            validY = validY[:, T, None]\n",
    "            testPred = testPred[:, T, None]\n",
    "            testY = testY[:, T, None]\n",
    "            #print(trainPred.shape)\n",
    "            #print(trainY.shape)\n",
    "        \n",
    "        # Invert Predictions\n",
    "        trainPred = Yscaler.inverse_transform(trainPred)\n",
    "        trainYinv = Yscaler.inverse_transform(trainY)\n",
    "        \n",
    "        validPred = Yscaler.inverse_transform(validPred)\n",
    "        validYinv = Yscaler.inverse_transform(validY)\n",
    "        \n",
    "        testPred = Yscaler.inverse_transform(testPred)\n",
    "        testYinv = Yscaler.inverse_transform(testY)\n",
    "        \n",
    "        # stats\n",
    "        num_train = len(trainX)\n",
    "        num_valid = len(validX)\n",
    "        num_test = len(testX)\n",
    "        \n",
    "        num_THETA_train = np.sum(trainYinv >= THETA)\n",
    "        num_THETA_valid = np.sum(validYinv >= THETA)\n",
    "        num_THETA_test = np.sum(testYinv >= THETA)\n",
    "        \n",
    "        scores_detail[\"train\"]['quantity'].append( (num_train, num_THETA_train) )\n",
    "        scores_detail[\"valid\"]['quantity'].append( (num_valid, num_THETA_valid) )\n",
    "        scores_detail[\"test\"]['quantity'].append(  (num_test, num_THETA_test)   )\n",
    "        \n",
    "        \n",
    "        # calculate root mean squared error\n",
    "        yt, yp = trainYinv, trainPred\n",
    "        scores = RMSE(yt, yp, THETA=THETA, ALL=True)\n",
    "        scores = [scores[\"RMSE\"], scores[\"RMSPE\"], scores[\"RMSEat\"], scores[\"RMSPEat\"] ]\n",
    "        for t in moreTHETA:\n",
    "            scores.append( RMSE(yt, yp, THETA=t) )\n",
    "            scores.append( RMSE(yt, yp, THETA=t, norm=True) )\n",
    "        trainRMSE.append( scores )\n",
    "        yt, yp = validYinv, validPred\n",
    "        scores = RMSE(yt, yp, THETA=THETA, ALL=True)\n",
    "        scores = [scores[\"RMSE\"], scores[\"RMSPE\"], scores[\"RMSEat\"], scores[\"RMSPEat\"] ]\n",
    "        for t in moreTHETA:\n",
    "            scores.append( RMSE(yt, yp, THETA=t) )\n",
    "            scores.append( RMSE(yt, yp, THETA=t, norm=True) )\n",
    "        validRMSE.append( scores )\n",
    "        yt, yp = testYinv, testPred\n",
    "        scores = RMSE(yt, yp, THETA=THETA, ALL=True)\n",
    "        scores = [scores[\"RMSE\"], scores[\"RMSPE\"], scores[\"RMSEat\"], scores[\"RMSPEat\"] ]\n",
    "        for t in moreTHETA:\n",
    "            scores.append( RMSE(yt, yp, THETA=t) )\n",
    "            scores.append( RMSE(yt, yp, THETA=t, norm=True) )\n",
    "        testRMSE.append( scores )\n",
    "        a,b = trainRMSE[-1][0:2]\n",
    "        c,d = validRMSE[-1][0:2]\n",
    "        e,f = testRMSE[-1][0:2]\n",
    "        print(\"fold %s score (RMSE,RMSPE): (%.2f, %.2f) (%.2f, %.2f) (%.2f, %.2f)\"%(fold, a,b,c,d,e,f))\n",
    "        \n",
    "        #scores = RMSE(validYinv, validPred, THETA=THETA, ALL=True)\n",
    "        #validRMSE.append( [scores[\"RMSE\"], scores[\"RMSPE\"], scores[\"RMSEat\"], scores[\"RMSPEat\"]] )\n",
    "        #scores = RMSE(testYinv, testPred, THETA=THETA, ALL=True)\n",
    "        #testRMSE.append(  [scores[\"RMSE\"], scores[\"RMSPE\"], scores[\"RMSEat\"], scores[\"RMSPEat\"]] )\n",
    "        \n",
    "        \n",
    "        #trainScore = RMSE(trainYinv, trainPred, 61)\n",
    "        #trainRMSE[-1].append( trainScore )\n",
    "        #validScore = RMSE(validYinv, validPred, 61)\n",
    "        #validRMSE[-1].append( validScore )\n",
    "        #testScore = RMSE(testYinv, testPred, 61)\n",
    "        #testRMSE[-1].append( testScore )\n",
    "        \n",
    "        \n",
    "        #MAE\n",
    "        yt, yp = trainYinv, trainPred\n",
    "        scores = MAE(yt, yp, THETA=THETA, ALL=True)\n",
    "        scores = [scores[\"MAE\"], scores[\"MAPE\"], scores[\"MAEat\"], scores[\"MAPEat\"] ]\n",
    "        for t in moreTHETA:\n",
    "            scores.append( MAE(yt, yp, THETA=t) )\n",
    "            scores.append( MAE(yt, yp, THETA=t, norm=True) )\n",
    "        trainMAE.append( scores )\n",
    "        #scores = MAE(trainYinv, trainPred, THETA=THETA, ALL=True)\n",
    "        #trainMAE.append( [scores[\"MAE\"], scores[\"MAPE\"], scores[\"MAEat\"], scores[\"MAPEat\"]] )\n",
    "        yt, yp = validYinv, validPred\n",
    "        scores = MAE(yt, yp, THETA=THETA, ALL=True)\n",
    "        scores = [scores[\"MAE\"], scores[\"MAPE\"], scores[\"MAEat\"], scores[\"MAPEat\"] ]\n",
    "        for t in moreTHETA:\n",
    "            scores.append( MAE(yt, yp, THETA=t) )\n",
    "            scores.append( MAE(yt, yp, THETA=t, norm=True) )\n",
    "        validMAE.append( scores )\n",
    "        #scores = MAE(validYinv, validPred, THETA=THETA, ALL=True)\n",
    "        #validMAE.append( [scores[\"MAE\"], scores[\"MAPE\"], scores[\"MAEat\"], scores[\"MAPEat\"]] )\n",
    "        yt, yp = testYinv, testPred\n",
    "        scores = MAE(yt, yp, THETA=THETA, ALL=True)\n",
    "        scores = [scores[\"MAE\"], scores[\"MAPE\"], scores[\"MAEat\"], scores[\"MAPEat\"] ]\n",
    "        for t in moreTHETA:\n",
    "            scores.append( MAE(yt, yp, THETA=t) )\n",
    "            scores.append( MAE(yt, yp, THETA=t, norm=True) )\n",
    "        testMAE.append( scores )\n",
    "        #scores = MAE(testYinv, testPred, THETA=THETA, ALL=True)\n",
    "        #testMAE.append( [scores[\"MAE\"], scores[\"MAPE\"], scores[\"MAEat\"], scores[\"MAPEat\"]] )\n",
    "        \n",
    "\n",
    "        if GRAPH == True:\n",
    "            print(\"Graficando...\")\n",
    "            df1 = join_index(dateTrain,trainYinv, \"Y\")\n",
    "            df2 = join_index(dateTrain,trainPred, \"f train\")\n",
    "            \n",
    "            df3 = join_index(dateValid,validYinv, \"Y\")\n",
    "            df4 = join_index(dateValid,validPred, \"f validation\")\n",
    "            \n",
    "            df5 = join_index(dateTest,testYinv, \"Y\")\n",
    "            df6 = join_index(dateTest,testPred, \"f test\")\n",
    "            \n",
    "            ydf = pd.concat([df1, df3, df5], axis=0)\n",
    "            df = pd.concat( [ydf, df4, df2, df6] ,axis=1)\n",
    "            \n",
    "            #traindf = pd.concat([df1,df2], axis=1)\n",
    "            #testdf = pd.concat([df3,df4], axis=1)\n",
    "            \n",
    "            #df = pd.concat([traindf,testdf])\n",
    "            \n",
    "            #fecha_test = testdf.index[0]\n",
    "            #df.asfreq(\"D\").iplot(title = \"%s - %s\"%(TARGET, LAYERS), vline=[fecha_test], hline=[THETA])\n",
    "            df.asfreq(\"D\").iplot(title = \"%s\"%(TARGET), hline=[THETA, df[\"Y\"].mean()], vline=[dateValid[0],dateTest[0]])\n",
    "            \n",
    "            df = df.asfreq(\"D\")\n",
    "            \n",
    "            list_df.append(df)\n",
    "        \n",
    "    print(\"### LSTM ###\")\n",
    "    \n",
    "    labels = [\"RMSE\", \"RMSPE\", \"RMSEat%s\"%THETA, \"RMSPEat%s\"%THETA]\n",
    "    for t in moreTHETA:\n",
    "        labels.append(\"RMSEat%s\"%t)\n",
    "        labels.append(\"RMSPEat%s\"%t)\n",
    "    maxlen = max(map(len,labels))\n",
    "    trainMeans = np.nanmean(trainRMSE, axis=0)\n",
    "    validMeans = np.nanmean(validRMSE, axis=0)\n",
    "    testMeans  = np.nanmean(testRMSE, axis=0)\n",
    "    print(\"{:>{maxlen}}{:>8}{:>8}{:>8}\".format(\"\",\"Train\",\"Valid\",\"Test\",maxlen=maxlen))\n",
    "    for i in range(len(labels)):\n",
    "        print( (\"{0:>{maxlen}}{1:8.2f}{2:8.2f}{3:8.2f}\".format (labels[i], trainMeans[i], validMeans[i], testMeans[i], maxlen=maxlen)) )\n",
    "        \n",
    "        scores_detail[\"train\"][labels[i]] = np.array(trainRMSE)[:,i]\n",
    "        scores_detail[\"valid\"][labels[i]] = np.array(validRMSE)[:,i]\n",
    "        scores_detail[\"test\"][labels[i]]  = np.array(testRMSE)[:,i]\n",
    "        \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    labels = [\"MAE\", \"MAPE\", \"MAEat%s\"%THETA, \"MAPEat%s\"%THETA]\n",
    "    for t in moreTHETA:\n",
    "        labels.append(\"RMSEat%s\"%t)\n",
    "        labels.append(\"RMSPEat%s\"%t)\n",
    "    maxlen = max(map(len,labels))\n",
    "    trainMeans = np.nanmean(trainMAE, axis=0)\n",
    "    validMeans = np.nanmean(validMAE, axis=0)\n",
    "    testMeans  = np.nanmean( testMAE, axis=0)\n",
    "    print(\"{:>{maxlen}}{:>8}{:>8}{:>8}\".format(\"\",\"Train\",\"Valid\",\"Test\",maxlen=maxlen))\n",
    "    for i in range(len(labels)):\n",
    "        print( (\"{0:>{maxlen}}{1:8.2f}{2:8.2f}{3:8.2f}\".format (labels[i], trainMeans[i], validMeans[i], testMeans[i], maxlen=maxlen)) )\n",
    "    \n",
    "        scores_detail[\"train\"][labels[i]] = np.array(trainMAE)[:,i]\n",
    "        scores_detail[\"valid\"][labels[i]] = np.array(validMAE)[:,i]\n",
    "        scores_detail[\"test\"][labels[i]]  = np.array(testMAE)[:,i]\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    \n",
    "    if FOLDS_TVT == False:\n",
    "        # PLOT\n",
    "        internalConfig[\"trainYinv\"] = trainYinv\n",
    "        internalConfig[\"validYinv\"] = validYinv\n",
    "        internalConfig[\"testYinv\"] = testYinv\n",
    "    else:\n",
    "        trainPred = \"DUMMY\"\n",
    "        validPred = \"DUMMY\"\n",
    "        testPred = \"DUMMY\"\n",
    "    \n",
    "    internalConfig[\"list_df\"] = list_df\n",
    "    \n",
    "    \n",
    "    return scores_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([ [1,2,3],\n",
    "               [4,5,6],\n",
    "               [7,8,9]])\n",
    "a[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## myLSTMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myLSTMModel(internalConfig, Config):\n",
    "    SEED = Config['SEED']\n",
    "    np.random.seed(SEED)\n",
    "    set_random_seed(SEED*SEED)\n",
    "    \n",
    "    ic = internalConfig\n",
    "    \n",
    "    TIMESTEP = Config['TIMESTEP']\n",
    "    TIMEDIST = Config[\"TIMEDIST\"]\n",
    "    FOLDS_TVT = Config[\"FOLDS_TVT\"]\n",
    "    PATIENCE = Config[\"PATIENCE\"]\n",
    "    OWN_SAVE = Config[\"OWN_SAVE\"]\n",
    "    OWN_LOAD = Config[\"OWN_LOAD\"]\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    OVERWRITE_MODEL = Config[\"OVERWRITE_MODEL\"]\n",
    "    TARGET = Config[\"TARGET\"]\n",
    "    BATCH_SIZE = Config[\"BATCH_SIZE\"]\n",
    "    THETA = Config[\"THETA\"]\n",
    "    LAYERS = Config[\"LAYERS\"]\n",
    "    DROP_RATE = Config[\"DROP_RATE\"]\n",
    "\n",
    "    EPOCHS = Config[\"EPOCHS\"]\n",
    "    LOSS = Config[\"LOSS\"]\n",
    "    f_len = ic[\"f_len\"]\n",
    "    y_len = ic[\"y_len\"]\n",
    "    \n",
    "    \n",
    "    if FOLDS_TVT == False:\n",
    "        #if TIMEDIST == True:\n",
    "        #    ic[\"trainY\"] = ic[\"trainY\"].reshape(-1, TIMESTEP, y_len )\n",
    "        #    ic[\"validY\"] = ic[\"validY\"].reshape(-1, TIMESTEP, y_len )\n",
    "        #    ic[\"testY\"] = ic[\"testY\"].reshape(-1, TIMESTEP, y_len )\n",
    "        #else:\n",
    "        #    ic[\"trainY\"] = ic[\"trainY\"][:,-1]\n",
    "        #    ic[\"validY\"] = ic[\"validY\"][:,-1]\n",
    "        #    ic[\"testY\"] = ic[\"testY\"][:,-1]\n",
    "        \n",
    "        list_trainX = [ ic[\"trainX\"] ]\n",
    "        list_trainY = [ ic[\"trainY\"] ]\n",
    "        list_validX = [ ic[\"validX\"] ]\n",
    "        list_validY = [ ic[\"validY\"] ]\n",
    "    else:\n",
    "        list_trainX = internalConfig['list_trainX']\n",
    "        list_trainY = internalConfig['list_trainY']\n",
    "        list_validX = internalConfig['list_validX']\n",
    "        list_validY = internalConfig['list_validY']\n",
    "    \n",
    "    \n",
    "    losses = []\n",
    "    list_models = []\n",
    "    \n",
    "    LAYERS = LAYERS[::-1]\n",
    "    DROP_RATE = DROP_RATE[::-1]\n",
    "    \n",
    "    for fold in range(0,len(list_trainX)):\n",
    "        internalConfig['fold'] = fold + 1\n",
    "        print(\"Using Fold index: %d/%d\"%(fold,len(list_trainX)-1) )\n",
    "        trainX = list_trainX[fold]\n",
    "        trainY = list_trainY[fold]\n",
    "        validX = list_validX[fold]\n",
    "        validY = list_validY[fold]\n",
    "        \n",
    "    \n",
    "        f_len = trainX.shape[-1]\n",
    "        print(\"trainX.shape,\",trainX.shape)\n",
    "        print(\"trainY.shape,\",trainY.shape)\n",
    "        print(\"validX.shape,\",validX.shape)\n",
    "        print(\"validY.shape,\",validY.shape)\n",
    "        \n",
    "    \n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=PATIENCE, restore_best_weights=True)\n",
    "        \n",
    "        FILE_NAME = create_filename(ic, Config)\n",
    "        \n",
    "        try:\n",
    "            if OVERWRITE_MODEL == False:\n",
    "                print(\"Loading Model...\")\n",
    "                if OWN_LOAD:\n",
    "                    if FOLDS_TVT == True:\n",
    "                        last_FILE_NAME = \"%s/%s-%s.h5\"%(MODELS_FOLDER,OWN_LOAD, fold)\n",
    "                        classicModel = load_model( last_FILE_NAME )\n",
    "                    else:\n",
    "                        last_FILE_NAME = \"%s/%s.h5\"%(MODELS_FOLDER,OWN_LOAD)\n",
    "                        classicModel = load_model( last_FILE_NAME )\n",
    "                else:\n",
    "                    classicModel = load_model(FILE_NAME)\n",
    "                    last_FILE_NAME = FILE_NAME\n",
    "                \n",
    "                list_models.append(classicModel)\n",
    "                print(FILE_NAME + \" loaded =)\")\n",
    "                print(\"LISTO\")\n",
    "            else:\n",
    "                print(\"Reentrenando \"+ FILE_NAME)\n",
    "                load_model(\"noexisto_nijamas_existire.h5\")\n",
    "        except:\n",
    "            print(\"batch_input_shape:(%d,%d,%d)\"%(BATCH_SIZE,TIMESTEP,f_len))\n",
    "            print(\"Loading Failed. Training again...\")\n",
    "            classicModel = Sequential()\n",
    "            \n",
    "            \n",
    "            ## SIN stateful\n",
    "            if len(LAYERS) == 2:\n",
    "                classicModel.add(LSTM(LAYERS[1], activation=\"sigmoid\", input_shape=(TIMESTEP, f_len), return_sequences=True))\n",
    "                classicModel.add(Dropout(rate=DROP_RATE[1]))\n",
    "            \n",
    "            if TIMEDIST == True:\n",
    "                classicModel.add(LSTM(LAYERS[0], activation=\"sigmoid\", input_shape=(TIMESTEP, f_len), return_sequences=True))\n",
    "                classicModel.add(Dropout(rate=DROP_RATE[0]))\n",
    "                classicModel.add( TimeDistributed( Dense( y_len, activation=\"linear\") ) )\n",
    "            else:\n",
    "                classicModel.add(LSTM(LAYERS[0], activation=\"sigmoid\", input_shape=(TIMESTEP, f_len), return_sequences=False))\n",
    "                classicModel.add(Dropout(rate=DROP_RATE[0]))\n",
    "                classicModel.add(Dense(y_len, activation=\"linear\"))\n",
    "            \n",
    "            if LOSS == \"\":\n",
    "                classicModel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "            elif LOSS == \"atTHETA\":\n",
    "                Yscaler = ic[\"scalers\"][\"Yscaler\"]\n",
    "                scaled_theta = Yscaler.transform([[ Config[\"THETA\"] ]])\n",
    "                classicModel.compile(loss=lambda y,f: lossAtTHETA(scaled_theta,y,f), optimizer='adam')\n",
    "            \n",
    "            classicModel.fit(trainX, trainY, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(validX,validY), callbacks=[es], verbose=1)#, shuffle=False)\n",
    "            \n",
    "            #classicModel.fit(trainX, trainY, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=2)\n",
    "            classicModel.save(FILE_NAME)\n",
    "            if FOLDS_TVT == True:\n",
    "                classicModel.save(\"%s/%s-%s.h5\"%(MODELS_FOLDER, OWN_SAVE, fold))\n",
    "            else:\n",
    "                classicModel.save(\"%s/%s-%s.h5\"%(MODELS_FOLDER, OWN_SAVE))\n",
    "            print(FILE_NAME + \" saved. = )\")\n",
    "            \n",
    "            list_models.append(classicModel)\n",
    "        \n",
    "    return list_models, FILE_NAME\n",
    "    #ic[\"model\"] = classicModel\n",
    "    #ic[\"file_name\"] = FILE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas\n",
    "Para cada configuración, si ya hay un modelo entrenado, éste será cargado, si no existe un modelo entrenado para dicha configuración, entonces se entrenará y guardará."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas con varias seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "STATION, FILTER_YEARS, THETA = get_station(\"Las_Condes\")\n",
    "LSTMconfig = {\n",
    "                    # import_merge_and_scale()\n",
    "                   \"STATION\" : STATION,\n",
    "                    \"SCALER\" : preprocessing.StandardScaler,\n",
    "                \"IMPUTATION\" : None,\n",
    "                 \"AGREGADOS\" : [],#[\"O3\",\"TEMP\",\"WS\",\"RH\"],    #[\"ALL\"] #Horas de los maximos que se quieren agregar.\n",
    "                    \"TARGET\" : \"O3\",\n",
    "                   \"PRECALC\" : precalcular_agregados(STATION),\n",
    "                     \"THETA\" : THETA,\n",
    "                 \"moreTHETA\" : [61],\n",
    "                    \"FUTURE\" : 1,\n",
    "                      \"PAST\" : False,\n",
    "                    \n",
    "                    # select_features()\n",
    "              \"FILTER_YEARS\" : FILTER_YEARS,#[2004,2013], #En Veranos\n",
    "                       \"CUT\" : 0.41,\n",
    "            \"FIXED_FEATURES\" : ['CO', 'PM10', 'PM25', 'NO', 'NOX', 'WD', 'RH', 'TEMP', 'WS', 'UVA', 'UVB', 'O3'], # empty list means that the CUT will be used\n",
    "                       \"BAN\" : [\"countEC\", \"EC\",\"O3btTHETA\"], # []\n",
    "                    \n",
    "                    #make_examples()\n",
    "                 \"FOLDS_TVT\" : True,\n",
    "                  \"TIMESTEP\" : 28,\n",
    "                   \"OVERLAP\" : True,\n",
    "                    \n",
    "                    #make_traintest() Ignored when FOLDS_TVT == True\n",
    "                   \"SHUFFLE\" : False,\n",
    "                  \"TRAINPCT\" : 0.85,\n",
    "                   \n",
    "                    \n",
    "                    #myLSTM()\n",
    "           \"OVERWRITE_MODEL\" : False,\n",
    "                \"MODEL_NAME\" : \"LSTM\",\n",
    "                    \"LAYERS\" : [18, 51],\n",
    "                  \"DROP_RATE\": [0.0970659473593013, 0.163032727137165],\n",
    "                \"BATCH_SIZE\" : 16,\n",
    "                    \"EPOCHS\" : 400,\n",
    "                  \"PATIENCE\" : 20,\n",
    "                  \"TIMEDIST\" : False,\n",
    "                      \"LOSS\" : \"\", # \"\" or \"atTHETA\"\n",
    "                     \"GRAPH\" : False,\n",
    "                  \"OWN_SAVE\" : None,\n",
    "                  \"OWN_LOAD\" : None, #\"MiModelo\",\n",
    "                    #Y to calc Error. For Test Only.\n",
    "                        \"Yx\" : 0    # DEFAULT 0\n",
    "                }\n",
    "\n",
    "seeds = [123, 57, 872, 340, 77, 583, 101, 178, 938, 555]\n",
    "all_scores = []\n",
    "all_outputs = []\n",
    "for s in seeds:\n",
    "    LSTMconfig[\"SEED\"] = s\n",
    "    myLSTMoutput = myLSTM(LSTMconfig)\n",
    "    all_outputs.append(myLSTMoutput)\n",
    "    all_scores.append( myLSTMPredict(myLSTMoutput, LSTMconfig) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"seeds-LSTM\")\n",
    "# promedio total sobre todas las seeds\n",
    "all_metrics = [\"RMSE\", \"RMSEat%s\"%THETA]\n",
    "for m in all_metrics:\n",
    "    for d in ['train', 'valid', 'test']:\n",
    "        fmeans = []\n",
    "        for i in range( len(seeds) ):\n",
    "            fmeans.append( np.nanmean(all_scores[i][d][m]) ) #mean over folds\n",
    "        mean = np.mean(fmeans)\n",
    "        std  = np.std(fmeans)\n",
    "        print(m, d, mean, std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "# promedio de cada fold sobre todas las seeds\n",
    "all_metrics = [\"RMSE\", \"RMSEat%s\"%THETA]\n",
    "for m in all_metrics:\n",
    "    for d in ['train', 'valid', 'test']:\n",
    "        for f in range( len(all_scores[0][d][m]) ):\n",
    "            fmeans = []\n",
    "            for i in range( len(seeds) ):\n",
    "                fmeans.append( all_scores[i][d][m][f] ) #mean over folds\n",
    "            mean = np.nanmean(fmeans)\n",
    "            std  = np.nanstd(fmeans)\n",
    "            #if d == 'test':\n",
    "            print(m, d, f, mean, std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\")\n",
    "# print de toda la info por folds\n",
    "all_metrics = [\"RMSE\", \"RMSEat%s\"%THETA]\n",
    "for m in all_metrics:\n",
    "    for d in ['train', 'valid', 'test']:\n",
    "        for f in range( len(all_scores[0][d][m]) ):\n",
    "            fmeans = []\n",
    "            for i in range( len(seeds) ):\n",
    "                print(m,d,all_scores[i][d][m])\n",
    "            print(\"####\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantile Regression LSTM multitask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make_qY\n",
    "Replica los array con las etiquetas Y las veces necesarias para entrenar el valor promedio y los cuantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify trainY and testY for Quantile Regression\n",
    "def make_qY(internalConfig, Config):\n",
    "    trainY = internalConfig[\"trainY\"]\n",
    "    validY = internalConfig[\"validY\"]\n",
    "    testY = internalConfig[\"testY\"]\n",
    "    QUANTILES = Config[\"QUANTILES\"]\n",
    "    TIMEDIST = Config[\"QUANTILES\"]\n",
    "    \n",
    "    print(\"Making Y for quantiles\")\n",
    "    ## Si hay problemas hacer train[:,None]\n",
    "    #print(trainY.shape)\n",
    "    #print(trainY[:,None].shape)\n",
    "    #trainYq = trainY[:,None]\n",
    "    #testYq = testY[:,None]\n",
    "    \n",
    "    if TIMEDIST == True:\n",
    "        pass\n",
    "    else:\n",
    "        trainYq = trainY[:, -1, :]\n",
    "        validYq = validY[:, -1, :]\n",
    "        testYq = testY[:, -1, :]\n",
    "        \n",
    "        trainY = trainY[:, -1, 0, None]\n",
    "        validY = validY[:, -1, 0, None]\n",
    "        testY = testY[:, -1, 0, None]\n",
    "        \n",
    "    \n",
    "        for i in range(len(QUANTILES)):\n",
    "            #trainYq = np.hstack([trainYq, trainY[:,None]])\n",
    "            #testYq = np.hstack([testYq, testY[:,None]])\n",
    "            trainYq = np.hstack([trainYq, trainY[:]])\n",
    "            validYq = np.hstack([validYq, validY[:]])\n",
    "            testYq = np.hstack([testYq, testY[:]])\n",
    "    \n",
    "    print(\"    trainYq.shape, \", trainYq.shape)\n",
    "    print(\"    testYq.shape, \", testYq.shape)\n",
    "    return trainYq, validYq, testYq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## quantil_loss\n",
    "Función de perdida para entrenar los cuantiles.  \n",
    "El valor total de la función contempla la salida del promedio y la de cada cuantil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "def quantil_loss(quantiles, ylen, qlen, ytrue, ypred):\n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(ylen):\n",
    "        loss += K.mean(K.square(ytrue[:,0] - ypred[:, i]), axis=-1)\n",
    "    \n",
    "    #loss = K.mean(K.square(ytrue[:, 0]-ypred[:, 0]), axis=-1)\n",
    "        \n",
    "    for k in range(qlen):\n",
    "        q = quantiles[k]\n",
    "        e = ( ytrue[:,0] - ypred[:, ylen+k])\n",
    "        loss += K.mean(q*e + K.clip(-e, K.epsilon(), np.inf), axis=-1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qmodel\n",
    "Función que entrena el modelo para predecir los cuantiles en el tiempo T+1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "def Qmodel(internalConfig, Config):#, trainX, trainY, ylen, FUTURE, PAST, TARGET, BATCH_SIZE, TIMESTEP, FEATURES, TRAINPCT, OVERWRITE_MODEL, MODEL_NAME, QUANTILES, LAYERS, EPOCHS, TIMEDIST):\n",
    "    SEED = Config['SEED']\n",
    "    np.random.seed(SEED)\n",
    "    set_random_seed(SEED*SEED)\n",
    "    \n",
    "    OVERWRITE_MODEL = Config[\"OVERWRITE_MODEL\"]\n",
    "    LAYERS = Config[\"LAYERS\"]\n",
    "    TIMESTEP = Config[\"TIMESTEP\"]\n",
    "    BATCH_SIZE = Config[\"BATCH_SIZE\"]\n",
    "    FOLDS_TVT = Config[\"FOLDS_TVT\"]\n",
    "    EPOCHS = Config[\"EPOCHS\"]\n",
    "    PATIENCE = Config[\"PATIENCE\"]\n",
    "    DROP_RATE = Config[\"DROP_RATE\"]\n",
    "    QUANTILES = Config[\"QUANTILES\"]\n",
    "    TIMEDIST = Config[\"TIMEDIST\"]\n",
    "    LOSS = Config[\"QLOSS\"]\n",
    "    y_len = internalConfig[\"y_len\"]\n",
    "    f_len = internalConfig[\"f_len\"]\n",
    "    \n",
    "    #trainX = internalConfig[\"trainX\"]\n",
    "    #trainY = internalConfig[\"trainY\"]\n",
    "    #validX = internalConfig[\"validX\"]\n",
    "    #validY = internalConfig[\"validY\"]\n",
    "    \n",
    "    if FOLDS_TVT == False:\n",
    "        list_trainX = [ internalConfig[\"trainX\"] ]\n",
    "        list_trainY = [ internalConfig[\"trainY\"] ]\n",
    "        list_validX = [ internalConfig[\"validX\"] ]\n",
    "        list_validY = [ internalConfig[\"validY\"] ]\n",
    "    else:\n",
    "        list_trainX = internalConfig['list_trainX']\n",
    "        list_trainY = internalConfig['list_trainY']\n",
    "        list_validX = internalConfig['list_validX']\n",
    "        list_validY = internalConfig['list_validY']\n",
    "    \n",
    "    \n",
    "    LAYERS.reverse()\n",
    "    DROP_RATE.reverse()\n",
    "    losses = []\n",
    "    list_models = []\n",
    "    for fold in range(0,len(list_trainX)):\n",
    "        internalConfig['fold'] = fold + 1\n",
    "        print(\"Using Fold:\", fold)\n",
    "        trainX = list_trainX[fold]\n",
    "        trainY = list_trainY[fold]\n",
    "        validX = list_validX[fold]\n",
    "        validY = list_validY[fold]\n",
    "        \n",
    "    \n",
    "        f_len = trainX.shape[-1]\n",
    "        \n",
    "        FILE_NAME = create_filename(internalConfig, Config)\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            print(\"Model File Name: \",FILE_NAME)\n",
    "            if OVERWRITE_MODEL == False:\n",
    "                print(\"Loading Model...\")\n",
    "                qModel = load_model(FILE_NAME, custom_objects = {'<lambda>': lambda y,f: LOSS(QUANTILES,y_len,len(QUANTILES),y,f)})\n",
    "                list_models.append(qModel)\n",
    "                #qModel.summary()\n",
    "                print(FILE_NAME + \" Loaded =)\")\n",
    "            else:\n",
    "                print(\"Reentrenando \"+ FILE_NAME)\n",
    "                load_model(\"noexisto_nijamas_existire.hacheCinco\")\n",
    "        except:\n",
    "            es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=PATIENCE)\n",
    "            qModel = Sequential()\n",
    "            if len(LAYERS) == 2:\n",
    "                qModel.add(LSTM(LAYERS[1], activation=\"sigmoid\", input_shape=(TIMESTEP, f_len), return_sequences=True))\n",
    "                qModel.add(Dropout(rate=DROP_RATE[1]))\n",
    "            \n",
    "            if TIMEDIST == True:\n",
    "                pass\n",
    "            else:\n",
    "                qModel.add(LSTM(LAYERS[0], activation=\"sigmoid\", input_shape=(TIMESTEP, f_len), return_sequences=False))\n",
    "                qModel.add(Dropout(rate=DROP_RATE[0]))\n",
    "                qModel.add(Dense( y_len + len(QUANTILES), activation=\"linear\" ))\n",
    "            \n",
    "            #qModel.summary()\n",
    "            qModel.compile(loss=lambda y,f: LOSS(QUANTILES, y_len,len(QUANTILES),y,f), optimizer='adam')\n",
    "            \n",
    "            qModel.fit(trainX, trainY, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(validX,validY), callbacks=[es], verbose=1)\n",
    "            \n",
    "            qModel.save(FILE_NAME)\n",
    "            print(FILE_NAME + \" Saved =)\")\n",
    "            \n",
    "            list_models.append(qModel)\n",
    "    \n",
    "    return list_models, FILE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qprediction\n",
    "Función que realiza predicciones para el modelo entrenado mediante la función Qmodel.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qprediction(internalConfig, Config):\n",
    "    QUANTILES = Config[\"QUANTILES\"]\n",
    "    BATCH_SIZE = Config[\"BATCH_SIZE\"]\n",
    "    THETA = Config[\"THETA\"]\n",
    "    moreTHETA = Config[\"moreTHETA\"]\n",
    "    TIMEDIST = Config[\"TIMEDIST\"]\n",
    "    FOLDS_TVT = Config[\"FOLDS_TVT\"]\n",
    "    GRAPH = Config[\"GRAPH\"]\n",
    "    #trainX = internalConfig[\"trainX\"]\n",
    "    #trainY = internalConfig[\"trainY\"]\n",
    "    #validX = internalConfig[\"validX\"]\n",
    "    #validY = internalConfig[\"validY\"]\n",
    "    #testX = internalConfig[\"testX\"]\n",
    "    #testY = internalConfig[\"testY\"]\n",
    "    Yscaler = internalConfig[\"scalers\"][\"Yscaler\"]\n",
    "    h24scaler = internalConfig[\"scalers\"][\"h24scaler\"]\n",
    "    list_models = internalConfig[\"list_models\"]\n",
    "    \n",
    "    \n",
    "    #WW = Config[\"WW\"]\n",
    "    \n",
    "    #if WW != None:\n",
    "    #    qModel.set_weights(WW)\n",
    "    \n",
    "    if FOLDS_TVT == False:\n",
    "        list_trainX = [ internalConfig[   \"trainX\"] ]\n",
    "        list_trainY = [ internalConfig[   \"trainY\"] ]\n",
    "        list_validX = [ internalConfig[   \"validX\"] ]\n",
    "        list_validY = [ internalConfig[   \"validY\"] ]\n",
    "        list_testX  = [ internalConfig[   \"testX\" ] ]\n",
    "        list_testY  = [ internalConfig[   \"testY\" ] ]\n",
    "        list_dateTrain = [ internalConfig[ \"dateTrain\"] ]\n",
    "        list_dateValid = [ internalConfig[ \"dateValid\"] ]\n",
    "        list_dateTest  = [ internalConfig[ \"dateTest\"] ] \n",
    "        \n",
    "    else:\n",
    "        list_trainX = internalConfig[   \"list_trainX\"]\n",
    "        list_trainY = internalConfig[   \"list_trainY\"]\n",
    "        list_validX = internalConfig[   \"list_validX\"]\n",
    "        list_validY = internalConfig[   \"list_validY\"]\n",
    "        list_testX  = internalConfig[   \"list_testX\"]\n",
    "        list_testY  = internalConfig[   \"list_testY\"]\n",
    "        list_dateTrain = internalConfig[\"list_dateTrain\"]\n",
    "        list_dateValid = internalConfig[\"list_dateValid\"]\n",
    "        list_dateTest  = internalConfig[\"list_dateTest\"]\n",
    "    \n",
    "    list_df = []\n",
    "    \n",
    "    # RMSE\n",
    "    trainRMSE = []\n",
    "    validRMSE = []\n",
    "    testRMSE  = []\n",
    "    \n",
    "    # MAE\n",
    "    trainMAE = []\n",
    "    validMAE = []\n",
    "    testMAE  = []\n",
    "    \n",
    "    scores_detail = {\n",
    "                \"train\": {'quantity':[]},\n",
    "                \"valid\": {'quantity':[]},\n",
    "                \"test\" : {'quantity':[]},\n",
    "                }\n",
    "    \n",
    "    # quantile metrics\n",
    "    trainQmetricAll =[]\n",
    "    validQmetricAll = []\n",
    "    testQmetricAll  = []\n",
    "    trainQmetricAtTHETA =[]\n",
    "    validQmetricAtTHETA = []\n",
    "    testQmetricAtTHETA  = []\n",
    "    \n",
    "    # Interval Coverage\n",
    "    trainIC = []\n",
    "    validIC = []\n",
    "    testIC  = []\n",
    "    \n",
    "    \n",
    "    print(\"Qprediction ...\")\n",
    "    for fold in range(0,len(list_trainX)):\n",
    "        trainX = list_trainX[fold]\n",
    "        trainY = list_trainY[fold]\n",
    "        validX = list_validX[fold]\n",
    "        validY = list_validY[fold]\n",
    "        testX  =  list_testX[fold]\n",
    "        testY  =  list_testY[fold]\n",
    "        dateTrain = list_dateTrain[fold]\n",
    "        dateValid = list_dateValid[fold]\n",
    "        dateTest  = list_dateTest[fold] \n",
    "    \n",
    "        qModel = list_models[fold]\n",
    "    \n",
    "        # Predictions\n",
    "        \n",
    "        qlen = len(QUANTILES)\n",
    "        trainPred = qModel.predict(trainX)\n",
    "        qModel.reset_states()\n",
    "        validPred = qModel.predict(validX)\n",
    "        qModel.reset_states()\n",
    "        testPred = qModel.predict(testX)\n",
    "        qModel.reset_states()\n",
    "        \n",
    "        if TIMEDIST == True:\n",
    "            pass\n",
    "        else:\n",
    "            #print(\"trainPred.shape, \",trainPred.shape)\n",
    "            #print(\"trainY.shape, \",trainY.shape)\n",
    "            #print(\"validPred.shape, \",validPred.shape)\n",
    "            #print(\"validY.shape, \",validY.shape)\n",
    "            #print(\"testPred.shape, \",testPred.shape)\n",
    "            #print(\"testY.shape, \",testY.shape)\n",
    "            #Reduces shape to ( examples, y + QUANTILES). Discarding y0, y2, y-1, etc...\n",
    "            trainPred = np.hstack( [trainPred[:,0,None], trainPred[:, -qlen:] ] )\n",
    "            validPred = np.hstack( [validPred[:,0,None], validPred[:, -qlen:] ] )\n",
    "            testPred  = np.hstack( [testPred[:,0,None],  testPred[:, -qlen:] ] )\n",
    "            #trainY = np.hstack( [trainY[:,0,None], trainY[:, -qlen:] ] )\n",
    "            #testY = np.hstack( [testY[:,0,None], testY[:, -qlen:] ] )\n",
    "            #print(\"trainPred.shape, \",trainPred.shape)\n",
    "            #print(\"trainY.shape, \",trainY.shape)\n",
    "            #print(\"testPred.shape, \",testPred.shape)\n",
    "            #print(\"testY.shape, \",testY.shape)\n",
    "        \n",
    "        \n",
    "        # Inverse Transform\n",
    "        finalTrainPredq = Yscaler.inverse_transform(trainPred[:,0, None])\n",
    "        #print(trainY.shape)\n",
    "        #print(trainY[:,0,None].shape)\n",
    "        finalTrainY = Yscaler.inverse_transform(trainY)\n",
    "        #print(finalTrainY.shape)\n",
    "        finalValidPredq = Yscaler.inverse_transform(validPred[:,0, None])\n",
    "        finalValidY = Yscaler.inverse_transform(validY)\n",
    "        finalTestPredq = Yscaler.inverse_transform(testPred[:,0,None])\n",
    "        finalTestY = Yscaler.inverse_transform(testY)\n",
    "        \n",
    "        \n",
    "        # stats\n",
    "        num_train = len(trainX)\n",
    "        num_valid = len(validX)\n",
    "        num_test = len(testX)\n",
    "        \n",
    "        num_THETA_train = np.sum(finalTrainY >= THETA)\n",
    "        num_THETA_valid = np.sum(finalValidY >= THETA)\n",
    "        num_THETA_test = np.sum(finalTestY >= THETA)\n",
    "        \n",
    "        scores_detail[\"train\"]['quantity'].append( (num_train, num_THETA_train) )\n",
    "        scores_detail[\"valid\"]['quantity'].append( (num_valid, num_THETA_valid) )\n",
    "        scores_detail[\"test\"]['quantity'].append(  (num_test, num_THETA_test)   )\n",
    "        \n",
    "        \n",
    "        # calculate root mean squared error\n",
    "        yt, yp = finalTrainY, finalTrainPredq\n",
    "        scores = RMSE(yt, yp, THETA=THETA, ALL=True)\n",
    "        scores = [scores[\"RMSE\"], scores[\"RMSPE\"], scores[\"RMSEat\"], scores[\"RMSPEat\"] ]\n",
    "        for t in moreTHETA:\n",
    "            scores.append( RMSE(yt, yp, THETA=t) )\n",
    "            scores.append( RMSE(yt, yp, THETA=t, norm=True) )\n",
    "        trainRMSE.append( scores )\n",
    "        yt, yp = finalValidY, finalValidPredq\n",
    "        scores = RMSE(yt, yp, THETA=THETA, ALL=True)\n",
    "        scores = [scores[\"RMSE\"], scores[\"RMSPE\"], scores[\"RMSEat\"], scores[\"RMSPEat\"] ]\n",
    "        for t in moreTHETA:\n",
    "            scores.append( RMSE(yt, yp, THETA=t) )\n",
    "            scores.append( RMSE(yt, yp, THETA=t, norm=True) )\n",
    "        validRMSE.append( scores )\n",
    "        yt, yp = finalTestY, finalTestPredq\n",
    "        scores = RMSE(yt, yp, THETA=THETA, ALL=True)\n",
    "        scores = [scores[\"RMSE\"], scores[\"RMSPE\"], scores[\"RMSEat\"], scores[\"RMSPEat\"] ]\n",
    "        for t in moreTHETA:\n",
    "            scores.append( RMSE(yt, yp, THETA=t) )\n",
    "            scores.append( RMSE(yt, yp, THETA=t, norm=True) )\n",
    "        testRMSE.append( scores )\n",
    "        a,b = trainRMSE[-1][0:2]\n",
    "        c,d = validRMSE[-1][0:2]\n",
    "        e,f = testRMSE[-1][0:2]\n",
    "        print(\"fold %s score (RMSE,RMSPE): (%.2f, %.2f) (%.2f, %.2f) (%.2f, %.2f)\"%(fold, a,b,c,d,e,f))\n",
    "        \n",
    "        '''\n",
    "        scores = RMSE(finalTrainY, finalTrainPredq, THETA=THETA, ALL=True)\n",
    "        trainRMSE.append( [scores[\"RMSE\"], scores[\"RMSPE\"], scores[\"RMSEat\"], scores[\"RMSPEat\"]] )\n",
    "        scores = RMSE(finalValidY, finalValidPredq, THETA=THETA, ALL=True)\n",
    "        validRMSE.append( [scores[\"RMSE\"], scores[\"RMSPE\"], scores[\"RMSEat\"], scores[\"RMSPEat\"]] )\n",
    "        scores = RMSE(finalTestY, finalTestPredq, THETA=THETA, ALL=True)\n",
    "        testRMSE.append(  [scores[\"RMSE\"], scores[\"RMSPE\"], scores[\"RMSEat\"], scores[\"RMSPEat\"]] )\n",
    "        \n",
    "        trainScore = RMSE(finalTrainY, finalTrainPredq, 61)\n",
    "        trainRMSE[-1].append( trainScore )\n",
    "        validScore = RMSE(finalValidY, finalValidPredq, 61)\n",
    "        validRMSE[-1].append( validScore )\n",
    "        testScore = RMSE(finalTestY, finalTestPredq, 61)\n",
    "        testRMSE[-1].append( testScore )\n",
    "        '''\n",
    "        \n",
    "        #MAE\n",
    "        yt, yp = finalTrainY, finalTrainPredq\n",
    "        scores = MAE(yt, yp, THETA=THETA, ALL=True)\n",
    "        scores = [scores[\"MAE\"], scores[\"MAPE\"], scores[\"MAEat\"], scores[\"MAPEat\"] ]\n",
    "        for t in moreTHETA:\n",
    "            scores.append( MAE(yt, yp, THETA=t) )\n",
    "            scores.append( MAE(yt, yp, THETA=t, norm=True) )\n",
    "        trainMAE.append( scores )\n",
    "        yt, yp = finalValidY, finalValidPredq\n",
    "        scores = MAE(yt, yp, THETA=THETA, ALL=True)\n",
    "        scores = [scores[\"MAE\"], scores[\"MAPE\"], scores[\"MAEat\"], scores[\"MAPEat\"] ]\n",
    "        for t in moreTHETA:\n",
    "            scores.append( MAE(yt, yp, THETA=t) )\n",
    "            scores.append( MAE(yt, yp, THETA=t, norm=True) )\n",
    "        validMAE.append( scores )\n",
    "        yt, yp = finalTestY, finalTestPredq\n",
    "        scores = MAE(yt, yp, THETA=THETA, ALL=True)\n",
    "        scores = [scores[\"MAE\"], scores[\"MAPE\"], scores[\"MAEat\"], scores[\"MAPEat\"] ]\n",
    "        for t in moreTHETA:\n",
    "            scores.append( MAE(yt, yp, THETA=t) )\n",
    "            scores.append( MAE(yt, yp, THETA=t, norm=True) )\n",
    "        testMAE.append( scores )\n",
    "        \n",
    "        '''\n",
    "        scores = MAE(finalTrainY, finalTrainPredq, THETA=THETA, ALL=True)\n",
    "        trainMAE.append( [scores[\"MAE\"], scores[\"MAPE\"], scores[\"MAEat\"], scores[\"MAPEat\"]] )\n",
    "        scores = MAE(finalValidY, finalValidPredq, THETA=THETA, ALL=True)\n",
    "        validMAE.append( [scores[\"MAE\"], scores[\"MAPE\"], scores[\"MAEat\"], scores[\"MAPEat\"]] )\n",
    "        scores = MAE(finalTestY, finalTestPredq, THETA=THETA, ALL=True)\n",
    "        testMAE.append( [scores[\"MAE\"], scores[\"MAPE\"], scores[\"MAEat\"], scores[\"MAPEat\"]] )\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        # quantile metric\n",
    "        trainMean , trainQMlosses = quantile_metrics(finalTrainY, QUANTILES, Yscaler.inverse_transform(trainPred[:,1:]))\n",
    "        trainQlosses = list(trainQMlosses) + [trainMean]\n",
    "        trainQmetricAll.append( trainQlosses )\n",
    "        validMean , validQMlosses = quantile_metrics(finalValidY, QUANTILES, Yscaler.inverse_transform(validPred[:,1:]))\n",
    "        validQlosses = list(validQMlosses) + [validMean]\n",
    "        validQmetricAll.append( validQlosses )\n",
    "        testMean , testQMlosses = quantile_metrics(finalTestY, QUANTILES, Yscaler.inverse_transform(testPred[:,1:]))\n",
    "        testQlosses = list(testQMlosses) + [testMean]\n",
    "        testQmetricAll.append( testQlosses )\n",
    "        \n",
    "        # Interval Coverage\n",
    "        bp = np.mean(Yscaler.inverse_transform(trainPred[:,0]))\n",
    "        nominal_sig = 1.0 - (QUANTILES[-1] - QUANTILES[0])\n",
    "        \n",
    "        inf_limit = Yscaler.inverse_transform(trainPred[:,1])\n",
    "        sup_limit = Yscaler.inverse_transform(trainPred[:,-1])\n",
    "        MIS, MSIS, cov_prob, lenght = IC_metrics(inf_limit, sup_limit, nominal_sig, finalTrainY, bp)\n",
    "        trainIC.append( [MIS, MSIS, cov_prob, lenght] )\n",
    "        \n",
    "        inf_limit = Yscaler.inverse_transform(validPred[:,1])\n",
    "        sup_limit = Yscaler.inverse_transform(validPred[:,-1])\n",
    "        MIS, MSIS, cov_prob, lenght = IC_metrics(inf_limit, sup_limit, nominal_sig, finalValidY, bp)\n",
    "        validIC.append( [MIS, MSIS, cov_prob, lenght] )\n",
    "        \n",
    "        inf_limit = Yscaler.inverse_transform(testPred[:,1])\n",
    "        sup_limit = Yscaler.inverse_transform(testPred[:,-1])\n",
    "        MIS, MSIS, cov_prob, lenght = IC_metrics(inf_limit, sup_limit, nominal_sig, finalTestY, bp)\n",
    "        testIC.append( [MIS, MSIS, cov_prob, lenght] )\n",
    "        \n",
    "        \n",
    "        # Only for graphics\n",
    "        if GRAPH == True:\n",
    "            for i in range(len(QUANTILES)):\n",
    "                temp = Yscaler.inverse_transform(trainPred[:,1+i,None])\n",
    "                finalTrainPredq = np.hstack([finalTrainPredq, temp])\n",
    "                #temp = Yscaler.inverse_transform(trainY[:,1+i,None])\n",
    "                #finalTrainY =np.hstack([finalTrainY, temp])\n",
    "                finalTrainY = Yscaler.inverse_transform(trainY)\n",
    "                \n",
    "                temp = Yscaler.inverse_transform(validPred[:,1+i,None])\n",
    "                finalValidPredq = np.hstack([finalValidPredq, temp])\n",
    "                finalValidY = Yscaler.inverse_transform(validY)\n",
    "                \n",
    "                temp = Yscaler.inverse_transform(testPred[:,1+i,None])\n",
    "                finalTestPredq = np.hstack([finalTestPredq, temp])\n",
    "                #temp = Yscaler.inverse_transform(testY[:,1+i,None])\n",
    "                #finalTestY = np.hstack([finalTestY, temp])\n",
    "                finalTestY = Yscaler.inverse_transform(testY)\n",
    "                \n",
    "            internalConfig[\"trainPred\"] = finalTrainPredq\n",
    "            internalConfig[\"trainYtrue\"] = finalTrainY\n",
    "            internalConfig[\"validPred\"] = finalValidPredq\n",
    "            internalConfig[\"validYtrue\"] = finalValidY\n",
    "            internalConfig[\"testPred\"] = finalTestPredq\n",
    "            internalConfig[\"testYtrue\"] = finalTestY\n",
    "            internalConfig[\"last_dateTrain\"] = dateTrain\n",
    "            internalConfig[\"last_dateValid\"] = dateValid\n",
    "            internalConfig[\"last_dateTest\"]  = dateTest\n",
    "            \n",
    "            df = graph_Qprediction(internalConfig, Config)\n",
    "            \n",
    "            list_df.append(df)\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    #print(finalTrainPredq.shape)\n",
    "        \n",
    "    \n",
    "    print(\"### LSTM CUANTÍLICA ###\")\n",
    "    labels = [\"RMSE\", \"RMSPE\", \"RMSEat%s\"%THETA, \"RMSPEat%s\"%THETA]\n",
    "    for t in moreTHETA:\n",
    "        labels.append(\"RMSEat%s\"%t)\n",
    "        labels.append(\"RMSPEat%s\"%t)\n",
    "    maxlen = max(map(len,labels))\n",
    "    trainMeans = np.nanmean(trainRMSE, axis=0)\n",
    "    validMeans = np.nanmean(validRMSE, axis=0)\n",
    "    testMeans  = np.nanmean(testRMSE, axis=0)\n",
    "    print(\"{:>{maxlen}}{:>8}{:>8}{:>8}\".format(\"\",\"Train\",\"Valid\",\"Test\",maxlen=maxlen))\n",
    "    for i in range(len(labels)):\n",
    "        print( (\"{0:>{maxlen}}{1:8.2f}{2:8.2f}{3:8.2f}\".format (labels[i], trainMeans[i], validMeans[i], testMeans[i], maxlen=maxlen)) )\n",
    "        \n",
    "        scores_detail[\"train\"][labels[i]] = np.array(trainRMSE)[:,i]\n",
    "        scores_detail[\"valid\"][labels[i]] = np.array(validRMSE)[:,i]\n",
    "        scores_detail[\"test\"][labels[i]]  = np.array(testRMSE)[:,i]\n",
    "        \n",
    "    print(\"\\n\")\n",
    "    '''\n",
    "    labels = [\"RMSE\", \"RMSPE\", \"RMSEat%s\"%THETA, \"RMSPEat%s\"%THETA, \"RMSEat61\"]\n",
    "    maxlen = max(map(len,labels))\n",
    "    trainMeans = np.nanmean(trainRMSE, axis=0)\n",
    "    validMeans = np.nanmean(validRMSE, axis=0)\n",
    "    testMeans  = np.nanmean(testRMSE, axis=0)\n",
    "    print(\"{:>{maxlen}}{:>8}{:>8}{:>8}\".format(\"\",\"Train\",\"Valid\",\"Test\",maxlen=maxlen))\n",
    "    for i in range(len(labels)):\n",
    "        print( (\"{0:>{maxlen}}{1:8.2f}{2:8.2f}{3:8.2f}\".format (labels[i], trainMeans[i], validMeans[i], testMeans[i], maxlen=maxlen)) )\n",
    "    print(\"\\n\")\n",
    "    '''\n",
    "    \n",
    "    labels = [\"MAE\", \"MAPE\", \"MAEat%s\"%THETA, \"MAPEat%s\"%THETA]\n",
    "    for t in moreTHETA:\n",
    "        labels.append(\"RMSEat%s\"%t)\n",
    "        labels.append(\"RMSPEat%s\"%t)\n",
    "    maxlen = max(map(len,labels))\n",
    "    trainMeans = np.nanmean(trainMAE, axis=0)\n",
    "    validMeans = np.nanmean(validMAE, axis=0)\n",
    "    testMeans  = np.nanmean( testMAE, axis=0)\n",
    "    print(\"{:>{maxlen}}{:>8}{:>8}{:>8}\".format(\"\",\"Train\",\"Valid\",\"Test\",maxlen=maxlen))\n",
    "    for i in range(len(labels)):\n",
    "        print( (\"{0:>{maxlen}}{1:8.2f}{2:8.2f}{3:8.2f}\".format (labels[i], trainMeans[i], validMeans[i], testMeans[i], maxlen=maxlen)) )\n",
    "    \n",
    "        scores_detail[\"train\"][labels[i]] = np.array(trainMAE)[:,i]\n",
    "        scores_detail[\"valid\"][labels[i]] = np.array(validMAE)[:,i]\n",
    "        scores_detail[\"test\"][labels[i]]  = np.array(testMAE)[:,i]\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    '''\n",
    "    labels = [\"MAE\", \"MAPE\", \"MAEat%s\"%THETA, \"MAPEat%s\"%THETA]\n",
    "    maxlen = max(map(len,labels))\n",
    "    trainMeans = np.nanmean(trainMAE, axis=0)\n",
    "    validMeans = np.nanmean(validMAE, axis=0)\n",
    "    testMeans  = np.nanmean( testMAE, axis=0)\n",
    "    print(\"{:>{maxlen}}{:>8}{:>8}{:>8}\".format(\"\",\"Train\",\"Valid\",\"Test\",maxlen=maxlen))\n",
    "    for i in range(len(labels)):\n",
    "        print( (\"{0:>{maxlen}}{1:8.2f}{2:8.2f}{3:8.2f}\".format (labels[i], trainMeans[i], validMeans[i], testMeans[i], maxlen=maxlen)) )\n",
    "    print(\"\\n\")\n",
    "    '''\n",
    "    \n",
    "    print(\"Quantile Metric\")\n",
    "    labels = QUANTILES + [\"mean\"]\n",
    "    trainMeans = np.nanmean(trainQmetricAll,axis=0)\n",
    "    validMeans = np.nanmean(validQmetricAll,axis=0)\n",
    "    testMeans  = np.nanmean(testQmetricAll,axis=0)\n",
    "    print(\"\\t\\tTrain\\tValid\\tTest\")\n",
    "    for i in range(len(labels)):\n",
    "        print(\"\\t%s\\t%.2f\\t%.2f\\t%.2f\" % (labels[i],trainMeans[i],validMeans[i],testMeans[i]))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    labels = [\"MIS\", \"MSIS\", \"cov_prob\", \"lenght\"]\n",
    "    trainMeans = np.nanmean(trainIC,axis=0)\n",
    "    validMeans = np.nanmean(validIC,axis=0)\n",
    "    testMeans  = np.nanmean(testIC ,axis=0)\n",
    "    print(\"{:>{maxlen}}{:>8}{:>8}{:>8}\".format(\"\",\"Train\",\"Valid\",\"Test\",maxlen=maxlen))\n",
    "    for i in range(len(labels)):\n",
    "        print( (\"{0:>{maxlen}}{1:8.2f}{2:8.2f}{3:8.2f}\".format (labels[i], trainMeans[i], validMeans[i], testMeans[i], maxlen=maxlen)) )\n",
    "    \n",
    "    #print(\"finalTrainPredq.shape, \",finalTrainPredq.shape)\n",
    "    #print(\"finalTrainY.shape, \",finalTrainY.shape)\n",
    "    #print(\"finalTestPredq.shape, \",finalTestPredq.shape)\n",
    "    #print(\"finalTestY.shape, \",finalTestY.shape)\n",
    "    #return (finalTrainPredq, finalTrainY, finalValidPredq, finalValidY, finalTestPredq, finalTestY)\n",
    "    \n",
    "    internalConfig[\"list_df\"] = list_df\n",
    "    \n",
    "    return scores_detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## graph_Qprediction\n",
    "Grafica las predicciones realizadas por la función Qprediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_Qprediction(internalConfig, Config):\n",
    "    QUANTILES = Config[\"QUANTILES\"]\n",
    "    THETA = Config[\"THETA\"]\n",
    "    \n",
    "    finalTrainPredq = internalConfig[\"trainPred\"]\n",
    "    finalTrainY = internalConfig[\"trainYtrue\"]\n",
    "    finalValidPredq = internalConfig[\"validPred\"]\n",
    "    finalValidY = internalConfig[\"validYtrue\"]\n",
    "    finalTestPredq = internalConfig[\"testPred\"]\n",
    "    finalTestY = internalConfig[\"testYtrue\"]\n",
    "    dateTrain = internalConfig[\"last_dateTrain\"]\n",
    "    dateValid = internalConfig[\"last_dateValid\"]\n",
    "    dateTest  = internalConfig[\"last_dateTest\"]\n",
    "    date_index = internalConfig[\"dateExamples\"]\n",
    "    \n",
    "    file_name = internalConfig[\"file_name\"]\n",
    "    dataset = internalConfig[\"complete_dataset\"]\n",
    "    Yscaler = internalConfig[\"scalers\"][\"Yscaler\"]\n",
    "    \n",
    "    \n",
    "    # Graph quantile TrainPred\n",
    "    #forPlot = np.hstack([dateTrain[:,None], finalTrainY[:,0,None] ])\n",
    "    \n",
    "    print(dateTrain[:,None].shape)\n",
    "    print(finalTrainY.shape)\n",
    "    \n",
    "    print(\"first dateTrain:\", dateTrain[0])\n",
    "    print(\"last dateTrain:\", dateTrain[-1])\n",
    "    print(\"first dateValid:\", dateValid[0])\n",
    "    print(\"last dateValid:\", dateValid[-1])\n",
    "    print(\"first dateTest:\", dateTest[0])\n",
    "    print(\"last dateTest:\", dateTest[-1])\n",
    "    \n",
    "    forPlot = np.hstack([dateTrain[:,None], finalTrainY ])\n",
    "    for c in finalTrainPredq.T[:,:,None]:\n",
    "        forPlot = np.hstack([forPlot, c])\n",
    "    #print(\"forPLot.shape, \", forPlot.shape)\n",
    "    df = pd.DataFrame(forPlot)\n",
    "    df.columns = [\"fecha\", \"y\", \"f\"] + list( map(str,QUANTILES) )\n",
    "    df = df.set_index(\"fecha\")\n",
    "    #print(df.asfreq(\"D\").iplot(title=FILE_NAME))\n",
    "    \n",
    "    #'''\n",
    "    # Graph quantile validPred\n",
    "    forPlot = np.hstack([dateValid[:,None], finalValidY ])\n",
    "    for c in finalValidPredq.T[:,:,None]:\n",
    "        forPlot = np.hstack([forPlot, c])\n",
    "    dv = pd.DataFrame(forPlot)\n",
    "    dv.columns = [\"fecha\", \"y\", \"f\"] + list( map(str,QUANTILES) )\n",
    "    dv = dv.set_index(\"fecha\")\n",
    "    #'''\n",
    "    \n",
    "    # Graph quantile TestPred\n",
    "    #forPlot = np.hstack([dateTest[:,None], finalTestY[:,0,None] ])\n",
    "    forPlot = np.hstack([dateTest[:,None], finalTestY ])\n",
    "    for c in finalTestPredq.T[:,:,None]:\n",
    "        forPlot = np.hstack([forPlot, c])\n",
    "    #print(\"forPLot.shape, \", forPlot.shape)\n",
    "    dd = pd.DataFrame(forPlot)\n",
    "    dd.columns = [\"fecha\", \"y\", \"f\"] + list( map(str,QUANTILES) )\n",
    "    dd = dd.set_index(\"fecha\")\n",
    "    #print(dd.asfreq(\"D\").iplot(title=FILE_NAME))\n",
    "    #print(dd.iplot(title=FILE_NAME))\n",
    "\n",
    "    #cc = pd.concat([df,dd], axis=0).drop(\"y\",axis=1)\n",
    "    cc = pd.concat([df,dv,dd], axis=0).drop(\"y\",axis=1)\n",
    "    i = date_index[0] + np.timedelta64(-1,\"D\")\n",
    "    f = date_index[-1] + np.timedelta64(-1,\"D\")\n",
    "\n",
    "    allY = dataset[\"y\"][i:f]\n",
    "    allY.index = allY.index + np.timedelta64(1,\"D\")\n",
    "    allY = pd.DataFrame(Yscaler.inverse_transform(allY[:,None]), index=allY.index)\n",
    "    allY.columns = [\"y\"]\n",
    "\n",
    "    hh = pd.concat([allY,cc], axis= 1)\n",
    "\n",
    "    hh.iplot(title=file_name, vline=[dateValid[0],dateTest[0]], hline=[THETA])\n",
    "    return hh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## quantileLSTM\n",
    "Llama a las otras funciones para entrenar un modelo, realizar predicciones y graficar.  \n",
    "Devuelve el modelo entrenado y un DataFrame con las predicciones para graficar directamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantileLSTM(Config):#, AGREGADOS, TARGET, THETA, FUTURE, PAST, FEATURES, CUT, BAN, TIMESTEP, OVERLAP, BATCH_SIZE, TRAINPCT, OVERWRITE_MODEL, MODEL_NAME, QUANTILES, LAYERS, EPOCHS, TIMEDIST):\n",
    "    #np.random.seed(123)\n",
    "    #set_random_seed(2)\n",
    "    \n",
    "    FOLDS_TVT = Config['FOLDS_TVT']\n",
    "    TIMESTEP = Config['TIMESTEP']\n",
    "    TIMEDIST = Config['TIMEDIST']\n",
    "    Config['SHIFT'] = Config['FUTURE'] * -1\n",
    "    \n",
    "    #SHIFT = FUTURE*-1\n",
    "    #PRECALC = precalcular_agregados()\n",
    "    #dataset, ylabels, Yscaler, h24scaler = import_merge_and_scale(AGREGADOS,TARGET, THETA, PRECALC, SHIFT, PAST)\n",
    "    \n",
    "    scalers = {}\n",
    "    ic = {\"scalers\": scalers}\n",
    "    \n",
    "    complete_dataset, ylabels, Yscaler, h24scaler = import_merge_and_scale(Config, verbose=False)\n",
    "    ic[\"complete_dataset\"] = complete_dataset\n",
    "    ic[\"ylabels\"] = ylabels\n",
    "    scalers['Yscaler'] = Yscaler\n",
    "    scalers[\"h24scaler\"] = h24scaler\n",
    "    y_len = len(ic[\"ylabels\"])\n",
    "    ic[\"y_len\"] = y_len\n",
    "    \n",
    "    data, features = select_features(ic, Config)\n",
    "    ic[\"data\"] = data\n",
    "    ic[\"features\"] = features\n",
    "    ic[\"f_len\"] = len(ic[\"features\"])\n",
    "    \n",
    "    ic[\"secuencias\"] = obtener_secuencias(ic)\n",
    "    \n",
    "    examples, y_examples, dateExamples = make_examples(ic, Config, verbose=False)\n",
    "    ic[\"examples\"] = examples\n",
    "    ic[\"y_examples\"] = y_examples\n",
    "    ic[\"dateExamples\"] = dateExamples\n",
    "    \n",
    "    if FOLDS_TVT == False:\n",
    "        tvtDict = make_traintest(ic, Config, verbose=False)\n",
    "        for key in tvtDict:\n",
    "            ic[key] = tvtDict[key]\n",
    "        \n",
    "    else:\n",
    "        list_tvtDict = make_folds_TVT(ic, Config)\n",
    "        for key in list_tvtDict:\n",
    "            ic[key] = list_tvtDict[key]\n",
    "    \n",
    "    #trainYq, testYq = make_qY(ic, Config)\n",
    "    #ic[\"trainYq\"] = trainYq\n",
    "    #ic[\"validYq\"] = validYq\n",
    "    #ic[\"testYq\"] = testYq\n",
    "\n",
    "    \n",
    "    \n",
    "    #trainYq = trainYq#.reshape(-1,5)\n",
    "    #print(\"trainYq.shape, \", trainYq.shape)\n",
    "    list_models, file_name = Qmodel(ic, Config)\n",
    "    if FOLDS_TVT == False:\n",
    "        ic[\"model\"] = list_models[0]\n",
    "        ic[\"list_models\"] = [ ic[\"model\"] ]\n",
    "    else:\n",
    "        ic[\"list_models\"] = list_models\n",
    "    \n",
    "    ic[\"file_name\"] = file_name\n",
    "    \n",
    "    \n",
    "    ##ftrp-> TRaining Prediction\n",
    "    ##ftry-> TRaining Y\n",
    "    ##ftep-> TEst Prediction\n",
    "    ##ftey-> TEst Y\n",
    "    #ftrp, ftry, fvap, fvay, ftep, ftey = Qprediction(ic, Config)\n",
    "    #ic[\"trainPred\"] = ftrp\n",
    "    #ic[\"trainYtrue\"] = ftry\n",
    "    #ic[\"validPred\"] = fvap\n",
    "    #ic[\"validYtrue\"] = fvay\n",
    "    #ic[\"testPred\"] = ftep\n",
    "    #ic[\"testYtrue\"] = ftey\n",
    "    \n",
    "    detail = Qprediction(ic, Config)\n",
    "    \n",
    "    ic[\"detail\"] = detail\n",
    "    \n",
    "    #if FOLDS_TVT == False:\n",
    "    #Qdf = graph_Qprediction(ic, Config)\n",
    "    #ic[\"df\"] = Qdf\n",
    "    \n",
    "    #scalers = {\"Yscaler\":Yscaler,\"h24scaler\":h24scaler}\n",
    "    #d = {\"data\":data, \"trainX\":trainX, \"trainY\":trainY, \"testX\":testX, \"testY\":testY, \"dateTrain\":dateTrain, \"dateTest\":dateTest,\n",
    "    #        \"trainPred\":ftrp, \"trainYtrue\":ftry, \"testPred\":ftep, \"testYtrue\":ftey,\n",
    "    #        \"modelName\":MODEL_NAME, \"scalers\":scalers}\n",
    "    return ic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas con varias seeds - preSQP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATION, FILTER_YEARS, THETA = get_station(\"Las_Condes\")\n",
    "qConfig = {\n",
    "                       \"STATION\" : STATION,\n",
    "                        \"SCALER\" : preprocessing.StandardScaler,\n",
    "                    \"IMPUTATION\" : None,\n",
    "                      \"AGREGADOS\":[],\n",
    "                         \"TARGET\": \"O3\",\n",
    "                       \"PRECALC\" : precalcular_agregados(STATION),\n",
    "                          \"THETA\": THETA,\n",
    "                     \"moreTHETA\" : [],\n",
    "                         \"FUTURE\": 1, #must be 1\n",
    "                           \"PAST\": False, #must be False\n",
    "                        \n",
    "                  \"FILTER_YEARS\" : FILTER_YEARS,\n",
    "                           \"CUT\" : 0.41,\n",
    "                \"FIXED_FEATURES\" : ['CO', 'PM10', 'PM25', 'NO', 'NOX', 'WD', 'RH', 'TEMP', 'WS', 'UVA', 'UVB', 'O3'], # empty list means that the CUT will be used\n",
    "                           \"BAN\" : [\"countEC\", \"EC\",\"O3btTHETA\"],\n",
    "                        \n",
    "                     \"FOLDS_TVT\" : True,\n",
    "                      \"TIMESTEP\" : 28,\n",
    "                        \"OVERLAP\": True,\n",
    "                     \n",
    "                       \"SHUFFLE\" : False,\n",
    "                       \"TRAINPCT\": 0.85,\n",
    "    \n",
    "                \"OVERWRITE_MODEL\": False,\n",
    "                    \"MODEL_NAME\" : \"qModel\",\n",
    "                      \"QUANTILES\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "                        \"LAYERS\" : [45],\n",
    "                      \"DROP_RATE\": [0.103909704884679],\n",
    "                          \"QLOSS\" : quantil_loss,\n",
    "                     \"BATCH_SIZE\": 16,\n",
    "                        \"EPOCHS\" : 400,\n",
    "                      \"PATIENCE\" : 20,\n",
    "                         \"GRAPH\" : False,\n",
    "                      \"TIMEDIST\" : False,  #Sin implementar\n",
    "    \n",
    "        }\n",
    "\n",
    "seeds = [123, 57, 872, 340, 77, 583, 101, 178, 938, 555]\n",
    "all_scoresQ = []\n",
    "all_outputsQ = []\n",
    "for s in seeds:\n",
    "    qConfig[\"SEED\"] = s\n",
    "    Qoutput = quantileLSTM(qConfig)\n",
    "    all_outputsQ.append(Qoutput)\n",
    "    all_scoresQ.append( Qprediction(Qoutput, qConfig) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"seeds-preSQP\")\n",
    "all_metrics = [\"RMSE\", \"RMSEat%s\"%THETA]\n",
    "for m in all_metrics:\n",
    "    for d in ['train', 'valid', 'test']:\n",
    "        fmeans = []\n",
    "        for i in range( len(seeds) ):\n",
    "            fmeans.append( np.nanmean(all_scoresQ[i][d][m]) ) #mean over folds\n",
    "        mean = np.mean(fmeans)\n",
    "        std  = np.std(fmeans)\n",
    "        print(m, d, mean, std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\")\n",
    "# promedio de cada fold sobre todas las seeds\n",
    "all_metrics = [\"RMSE\", \"RMSEat%s\"%THETA]\n",
    "for m in all_metrics:\n",
    "    for d in ['train', 'valid', 'test']:\n",
    "        for f in range( len(all_scoresQ[0][d][m]) ):\n",
    "            fmeans = []\n",
    "            for i in range( len(seeds) ):\n",
    "                fmeans.append( all_scoresQ[i][d][m][f] ) #mean over folds\n",
    "            mean = np.nanmean(fmeans)\n",
    "            std  = np.nanstd(fmeans)\n",
    "            #if d == 'test':\n",
    "            print(m, d, f, mean, std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, randint, quniform\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def data():\n",
    "    STATION, FILTER_YEARS, THETA = get_station(\"POH_full\")\n",
    "    Config = {\n",
    "                    # import_merge_and_scale()\n",
    "                   \"STATION\" : STATION,\n",
    "                    \"SCALER\" : preprocessing.StandardScaler,\n",
    "                 \"AGREGADOS\" : [],#[\"O3\",\"TEMP\",\"WS\",\"RH\"],    #[\"ALL\"] #Horas de los maximos que se quieren agregar.\n",
    "                \"IMPUTATION\" : None,\n",
    "                    \"TARGET\" : \"O3\",\n",
    "                   \"PRECALC\" : [],#precalcular_agregados(),\n",
    "                     \"THETA\" : THETA,\n",
    "                 \"moreTHETA\" : [],\n",
    "                    \"FUTURE\" : 1,\n",
    "                      \"PAST\" : False,\n",
    "              \"FILTER_YEARS\" : FILTER_YEARS,\n",
    "                    \n",
    "                    # select_features()\n",
    "                       \"CUT\" : 0.41, #relacionado con FILTER_YEARS\n",
    "            \"FIXED_FEATURES\" : ['CO', 'PM10', 'PM25', 'NO', 'NOX', 'WD', 'RH', 'TEMP', 'WS', 'UVA', 'UVB', 'O3'], # empty list means that the CUT will be used\n",
    "                       \"BAN\" : [\"countEC\", \"EC\",\"O3btTHETA\"], # []\n",
    "                    \n",
    "                    #make_examples()\n",
    "                 \"FOLDS_TVT\" : True,\n",
    "                   \"OVERLAP\" : True,\n",
    "                     \n",
    "                     \"GRAPH\" : False,\n",
    "                        \"Yx\" : 0    # DEFAULT 0\n",
    "                }\n",
    "    \n",
    "    np.random.seed(123)\n",
    "    Config['SHIFT'] = Config['FUTURE'] * -1\n",
    "    \n",
    "    scalers = {}\n",
    "    ic = {\"scalers\": scalers}\n",
    "    \n",
    "    complete_dataset, ylabels, Yscaler, h24scaler = import_merge_and_scale(Config, verbose=False)\n",
    "    ic[\"complete_dataset\"] = complete_dataset\n",
    "    ic[\"ylabels\"] = ylabels\n",
    "    scalers['Yscaler'] = Yscaler\n",
    "    scalers[\"h24scaler\"] = h24scaler\n",
    "    \n",
    "    y_len = len(ic[\"ylabels\"])\n",
    "    ic[\"y_len\"] = y_len\n",
    "    \n",
    "    data, features = select_features(ic, Config)\n",
    "    ic[\"data\"] = data\n",
    "    ic[\"features\"] = features\n",
    "    \n",
    "    ic[\"f_len\"] = len(ic[\"features\"])\n",
    "    \n",
    "    ic[\"secuencias\"] = obtener_secuencias(ic)\n",
    "    \n",
    "    \n",
    "    dtrainX = {}\n",
    "    dtrainY = {}\n",
    "    dvalidX = {}\n",
    "    dvalidY = {}\n",
    "    dtestX = {}\n",
    "    dtestY = {}\n",
    "    ddateTrain = {}\n",
    "    ddateValid = {}\n",
    "    ddateTest = {} \n",
    "    for LAGS in [1, 2, 3, 5, 7, 14, 21, 28]:\n",
    "        Config[\"TIMESTEP\"] = LAGS\n",
    "        examples, y_examples, dateExamples = make_examples(ic, Config, verbose=False)\n",
    "        ic[\"examples\"] = examples\n",
    "        ic[\"y_examples\"] = y_examples\n",
    "        ic[\"dateExamples\"] = dateExamples\n",
    "        \n",
    "        d = make_folds_TVT(ic, Config)\n",
    "        dtrainX[LAGS] = d[\"list_trainX\"]\n",
    "        dtrainY[LAGS] = d[\"list_trainY\"]\n",
    "        dvalidX[LAGS] = d[\"list_validX\"]\n",
    "        dvalidY[LAGS] = d[\"list_validY\"]\n",
    "        dtestX[LAGS]  = d[\"list_testX\"]\n",
    "        dtestY[LAGS]  = d[\"list_testY\"]\n",
    "        ddateTrain[LAGS] = d[\"list_dateTrain\"]\n",
    "        ddateValid[LAGS] = d[\"list_dateValid\"]\n",
    "        ddateTest[LAGS]  = d[\"list_dateTest\"]\n",
    "            \n",
    "    all_data = {\n",
    "                \"trainX\":dtrainX,\n",
    "                \"trainY\":dtrainY,\n",
    "                \"validX\":dvalidX,\n",
    "                \"validY\":dvalidY,\n",
    "                \"testX\" :dtestX,\n",
    "                \"testY\" :dtestY,\n",
    "             \"dateTrain\": ddateTrain,\n",
    "             \"dateValid\": ddateValid,\n",
    "              \"dateTest\": ddateTest,\n",
    "                }\n",
    "            \n",
    "    filename = 'dictDATA%s.pkl'%(Config[\"IMPUTATION\"])\n",
    "    output = open(filename, 'wb')\n",
    "    pickle.dump(all_data, output)\n",
    "    output.close()\n",
    "    \n",
    "    return all_data, scalers, Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HyperData():\n",
    "    pkl_file = open('dictDATANone.pkl', 'rb')\n",
    "    all_data = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "    dtrainX=all_data[\"trainX\"]\n",
    "    dtrainY=all_data[\"trainY\"]\n",
    "    dvalidX=all_data[\"validX\"]\n",
    "    dvalidY=all_data[\"validY\"]\n",
    "    \n",
    "    return dtrainX, dtrainY, dvalidX, dvalidY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HyperData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_HyperLSTM_Config():\n",
    "    Config = {\n",
    "                    #myLSTM()\n",
    "                \"BATCH_SIZE\" : 16,\n",
    "                    \"EPOCHS\" : 400,\n",
    "                  \"PATIENCE\" : 20,\n",
    "                }\n",
    "    return Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HyperLSTM(dtrainX, dtrainY, dvalidX, dvalidY):\n",
    "    Config = {\n",
    "                    # import_merge_and_scale()\n",
    "                \"BATCH_SIZE\" : 16,\n",
    "                    \n",
    "                    #myLSTM()\n",
    "                    \"EPOCHS\" : 400,\n",
    "                  \"PATIENCE\" : 20,\n",
    "                }\n",
    "    \n",
    "    \n",
    "    Config = get_HyperLSTM_Config()\n",
    "        \n",
    "    #print(f_len)\n",
    "    BATCH_SIZE = Config[\"BATCH_SIZE\"]\n",
    "    PATIENCE = Config[\"PATIENCE\"]\n",
    "    EPOCHS = Config[\"EPOCHS\"]\n",
    "    \n",
    "    LAGS = {{choice([1, 2, 3, 5, 7, 14, 21, 28])}}\n",
    "    TIMESTEP = LAGS\n",
    "    print(\"LAGS,\", LAGS)\n",
    "    \n",
    "    list_trainX = dtrainX[LAGS]\n",
    "    list_trainY = dtrainY[LAGS]\n",
    "    list_validX = dvalidX[LAGS]\n",
    "    list_validY = dvalidY[LAGS]\n",
    "    \n",
    "    #print(\"SPACE\")\n",
    "    #for k in space:\n",
    "    #    print(k,space[k])\n",
    "    \n",
    "    \n",
    "    \n",
    "    losses = []\n",
    "    list_models = []\n",
    "    for fold in range(0,len(list_trainX)):\n",
    "        print(\"Using Fold %s/%s:\"%(fold,len(list_trainX)-1))\n",
    "        trainX = list_trainX[fold]\n",
    "        trainY = list_trainY[fold]\n",
    "        validX = list_validX[fold]\n",
    "        validY = list_validY[fold]\n",
    "        \n",
    "    \n",
    "        f_len = trainX.shape[-1]\n",
    "        print(\"trainX.shape,\",trainX.shape)\n",
    "        print(\"trainY.shape,\",trainY.shape)\n",
    "        print(\"validX.shape,\",validX.shape)\n",
    "        print(\"validY.shape,\",validY.shape)\n",
    "        \n",
    "    \n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=PATIENCE, restore_best_weights=True)\n",
    "        model = Sequential()\n",
    "        layers={{choice([\"one\", \"two\"])}}\n",
    "        print(layers)\n",
    "        if  layers == 'two':\n",
    "            model.add(LSTM(round({{uniform(0,1)}}*10000)%(LAGS*2)+1, activation=\"sigmoid\", input_shape=(TIMESTEP, f_len), return_sequences=True))\n",
    "            model.add(Dropout(rate={{uniform(0, 1)}}))\n",
    "        \n",
    "        model.add(LSTM(round({{uniform(0,1)}}*10000)%(LAGS*2)+1, activation=\"sigmoid\", input_shape=(TIMESTEP, f_len), return_sequences=False))\n",
    "        model.add(Dropout(rate={{uniform(0, 1)}}))\n",
    "        model.add(Dense( 1, activation='linear'))\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam' )\n",
    "        model.fit( trainX, trainY, epochs=EPOCHS, validation_data=(validX, validY), batch_size=BATCH_SIZE, callbacks=[es], verbose=2)\n",
    "        loss = model.evaluate(validX, validY, verbose=1)\n",
    "        print(\"fold: \",fold, \"    loss:\",loss)\n",
    "        losses.append(loss)\n",
    "        list_models.append(model)\n",
    "        print(\"Hora de termino: \",str(datetime.now()))\n",
    "    \n",
    "    meanloss = sum(losses)/len(losses)\n",
    "    print('Valid Loss:', meanloss)\n",
    "    return {'loss': meanloss, 'status': STATUS_OK, 'model': list_models}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperPreSQP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preSQP_Config():\n",
    "    Config = {\n",
    "                    #myLSTM()\n",
    "                \"BATCH_SIZE\" : 16,\n",
    "                    \"EPOCHS\" : 400,\n",
    "                  \"PATIENCE\" : 20,\n",
    "                 \"QUANTILES\" : [ 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7 ,0.8 ,0.9],\n",
    "                 #\"QUANTILES\" : [ 0.75],\n",
    "                }\n",
    "    return Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HyperPreSQP(dtrainX, dtrainY, dvalidX, dvalidY):\n",
    "    Config = get_preSQP_Config()\n",
    "    BATCH_SIZE = Config[\"BATCH_SIZE\"]\n",
    "    EPOCHS = Config[\"EPOCHS\"]\n",
    "    PATIENCE = Config[\"PATIENCE\"]\n",
    "    QUANTILES = Config[\"QUANTILES\"]\n",
    "    \n",
    "    LAGS = {{choice([1, 2, 3, 5, 7, 14, 21, 28])}}\n",
    "    TIMESTEP = LAGS\n",
    "    print(\"LAGS,\", LAGS)\n",
    "    \n",
    "    list_trainX = dtrainX[LAGS]\n",
    "    list_trainY = dtrainY[LAGS]\n",
    "    list_validX = dvalidX[LAGS]\n",
    "    list_validY = dvalidY[LAGS]\n",
    "    \n",
    "    #print(\"SPACE\")\n",
    "    #for k in space:\n",
    "    #    print(k,space[k])\n",
    "\n",
    "    qlen = len(QUANTILES)\n",
    "    \n",
    "    losses = []\n",
    "    list_models = []\n",
    "    for fold in range(0,len(list_trainX)):\n",
    "        print(\"Using Fold %s-%s:\"%(fold,len(list_trainX)-1))\n",
    "        trainX = list_trainX[fold]\n",
    "        trainY = list_trainY[fold]\n",
    "        validX = list_validX[fold]\n",
    "        validY = list_validY[fold]\n",
    "        \n",
    "    \n",
    "        f_len = trainX.shape[-1]\n",
    "        print(\"trainX.shape,\",trainX.shape)\n",
    "        print(\"trainY.shape,\",trainY.shape)\n",
    "        print(\"validX.shape,\",validX.shape)\n",
    "        print(\"validY.shape,\",validY.shape)\n",
    "        \n",
    "    \n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=PATIENCE, restore_best_weights=True)\n",
    "        qModel = Sequential()\n",
    "        layers={{choice([\"one\", \"two\"])}}\n",
    "        print(layers)\n",
    "        if  layers == 'two':\n",
    "            qModel.add(LSTM(round({{uniform(0,1)}}*10000)%(LAGS*2)+1, activation=\"sigmoid\", input_shape=(TIMESTEP, f_len), return_sequences=True))\n",
    "            qModel.add(Dropout(rate={{uniform(0, 1)}}))\n",
    "        \n",
    "        qModel.add(LSTM(round({{uniform(0,1)}}*10000)%(LAGS*2)+1, activation=\"sigmoid\", input_shape=(TIMESTEP, f_len), return_sequences=False))\n",
    "        qModel.add(Dropout(rate={{uniform(0, 1)}}))\n",
    "        qModel.add(Dense( 1 + len(QUANTILES), activation=\"linear\" ))\n",
    "        \n",
    "        qModel.compile(loss=lambda y,f: quantil_loss(QUANTILES, 1,qlen,y,f), optimizer='adam')\n",
    "        \n",
    "        qModel.fit(trainX, trainY, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(validX,validY), callbacks=[es], verbose=2)\n",
    "        \n",
    "        loss = qModel.evaluate(validX, validY, verbose=2)\n",
    "        print(\"fold: \",fold, \"    loss:\",loss)\n",
    "        losses.append(loss)\n",
    "        list_models.append(qModel)\n",
    "        print(\"Hora de termino: \",str(datetime.now()))\n",
    "    \n",
    "    meanloss = sum(losses)/len(losses)\n",
    "    print('Valid Loss:', meanloss)\n",
    "    return {'loss': meanloss, 'status': STATUS_OK, 'model': list_models}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"-f\" not in sys.argv:\n",
    "    MODEL_NAME = sys.argv[1]\n",
    "    MAX_EVALS = int(sys.argv[2])\n",
    "    print(\"MODEL_NAME:\", MODEL_NAME)\n",
    "    print(\"MAX_EVALS:\", MAX_EVALS)\n",
    "else:\n",
    "    MODEL_NAME = \"preSQP\"\n",
    "    MAX_EVALS = 1\n",
    "\n",
    "\n",
    "if MODEL_NAME == \"HyperLSTM\":\n",
    "    HyperModel = HyperLSTM\n",
    "    get_Config = get_HyperLSTM_Config\n",
    "    FUNCTIONS = [get_HyperLSTM_Config]\n",
    "\n",
    "elif MODEL_NAME == \"preSQP\":\n",
    "    HyperModel = HyperPreSQP\n",
    "    get_Config = get_preSQP_Config\n",
    "    FUNCTIONS = [get_preSQP_Config, quantil_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, best_model = optim.minimize(model= HyperModel,\n",
    "                                          data= HyperData,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          functions = FUNCTIONS,\n",
    "                                          max_evals=MAX_EVALS,\n",
    "                                          trials=Trials(),\n",
    "                                          notebook_name= \"ozone_forecasting_multi-task\" if \"-f\" in sys.argv else None\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_NAME == \"HyperLSTM\":\n",
    "    LAGS = [1, 2, 3, 5, 7, 14, 21, 28][best_run['LAGS']]\n",
    "    layers = ['one', 'two'][best_run['layers']]\n",
    "    lstm1 = round(best_run['round']*10000)%(LAGS*2)+1\n",
    "    dropout1 = best_run['rate']\n",
    "    lstm2 = round(best_run['round_1']*10000)%(LAGS*2)+1\n",
    "    dropout2 = best_run['rate_1']\n",
    "    \n",
    "    print(\"Configuracion:\")\n",
    "    print(\"        LAGS:\",LAGS)\n",
    "    print(\"      layers:\",layers)\n",
    "    print(\"       lstm1:\",lstm1)\n",
    "    print(\"    dropout1:\",dropout1)\n",
    "    print(\"       lstm2:\",lstm2)\n",
    "    print(\"    dropout2:\",dropout2)\n",
    "    print(\"\")\n",
    "    \n",
    "    Config = a[2] #Config de data()\n",
    "    scalers = a[1]\n",
    "    modelConfig = get_Config()\n",
    "    Config[\"TIMEDIST\"] = LAGS\n",
    "    Config[\"BATCH_SIZE\"] = modelConfig[\"BATCH_SIZE\"]\n",
    "    ic = {}\n",
    "    ic[\"scalers\"] = scalers\n",
    "    ic[\"list_models\"] = best_model\n",
    "    for k in a[0]:\n",
    "        ic[\"list_\"+k] = a[0][k][LAGS]\n",
    "\n",
    "    myLSTMPredict(ic,Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_NAME == \"preSQP\":\n",
    "    LAGS = [1, 2, 3, 5, 7, 14, 21, 28][best_run['LAGS']]\n",
    "    #LAGS = [1, 2, 3, 4, 5][best_run['LAGS']]\n",
    "    layers = ['one', 'two'][best_run['layers']]\n",
    "    lstm1 = round(best_run['round']*10000)%(LAGS*2)+1\n",
    "    dropout1 = best_run['rate']\n",
    "    lstm2 = round(best_run['round_1']*10000)%(LAGS*2)+1\n",
    "    dropout2 = best_run['rate_1']\n",
    "    modelConfig = get_Config()\n",
    "    QUANTILES = modelConfig[\"QUANTILES\"]\n",
    "    \n",
    "    print(\"Configuracion:\")\n",
    "    print(\"         LAGS:\",LAGS)\n",
    "    print(\"    QUANTILES:\",QUANTILES)\n",
    "    print(\"       layers:\",layers)\n",
    "    print(\"        lstm1:\",lstm1)\n",
    "    print(\"     dropout1:\",dropout1)\n",
    "    print(\"        lstm2:\",lstm2)\n",
    "    print(\"     dropout2:\",dropout2)\n",
    "    print(\"\")\n",
    "    \n",
    "    Config = a[2] #Config de data()\n",
    "    scalers = a[1]\n",
    "    \n",
    "    Config[\"TIMEDIST\"] = LAGS\n",
    "    Config[\"QUANTILES\"] = QUANTILES\n",
    "    Config[\"BATCH_SIZE\"] = modelConfig[\"BATCH_SIZE\"]\n",
    "    ic = {}\n",
    "    ic[\"scalers\"] = scalers\n",
    "    ic[\"list_models\"] = best_model\n",
    "    for k in a[0]:\n",
    "        ic[\"list_\"+k] = a[0][k][LAGS]\n",
    "    \n",
    "    Qprediction(ic, Config)\n",
    "    print(\"mean_loss2, meanerror + meanLq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
